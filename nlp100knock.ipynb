{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP; Natural Language Processing 100 KNOCKs!!!!\n",
    "https://nlp100.github.io/ja/\n",
    "## 配布しているデータについて\n",
    "- baby-names.txt: 米国社会保障局 (SSA: Social Security Administration)のウェブサイト”Beyond the Top 1000 Names“で公開されている全州のデータを加工し，TSV形式に変換したもの．\n",
    "- jawiki-country.json.gz: 2020年4月5日付けの日本語のWikipedia記事のダンプの中から，国家に言及していると思われる記事を抽出し，JSON形式で格納したものです．このファイルは，クリエイティブ・コモンズ 表示-継承 3.0 非移植のライセンスで配布されています．\n",
    "- neko.txt: 青空文庫で公開されている夏目漱石の長編小説『吾輩は猫である』をテキストファイルに整形したものです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Processing ./.cache/pip/wheels/89/17/dd/5581d8a65aeb283780aba4d50e40ba74e695fc0e9948756082/japanize_matplotlib-1.1.2-py3-none-any.whl\nRequirement already satisfied: matplotlib in /root/local/python-3.8.3/lib/python3.8/site-packages (from japanize_matplotlib) (3.2.1)\nRequirement already satisfied: cycler>=0.10 in /root/local/python-3.8.3/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /root/local/python-3.8.3/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /root/local/python-3.8.3/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /root/local/python-3.8.3/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.2.0)\nRequirement already satisfied: numpy>=1.11 in /root/local/python-3.8.3/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.19.1)\nRequirement already satisfied: six in /root/local/python-3.8.3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->japanize_matplotlib) (1.15.0)\nInstalling collected packages: japanize-matplotlib\nSuccessfully installed japanize-matplotlib-1.1.2\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/root/local/python-3.8.3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "try:\n",
    "    import japanize_matplotlib\n",
    "except ModuleNotFoundError:\n",
    "    !pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第１章 準備運動\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. 文字列の逆順 \n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"stressed\"\n",
    "print(s[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 「パタトクカシーー」 \n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"パタトクカシー\"\n",
    "s[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」 \n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"パトカー\"\n",
    "s2 = \"タクシー\"\n",
    "s = \"\"\n",
    "for i in range(4):\n",
    "    s += s1[i]+s2[i]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 円周率 \n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "sentence = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "word_len = [len(word) for word in re.sub(\"[,.]\", \"\", sentence).split(\" \")]\n",
    "#======------======#\n",
    "print(sentence)\n",
    "print(re.sub(\"[,.]\", \"\", sentence))\n",
    "print(re.sub(\"[,.]\", \"\", sentence).split(\" \"))\n",
    "print(word_len)\n",
    "print(np.round(np.pi, len(re.sub(\"[,.]\", \"\", sentence).split(\" \"))-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 元素記号 \n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭に2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "words    = re.sub(\"[,.]\", \"\", sentence).split(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_1_idxs = [1,5,6,7,8,9,15,16,19]\n",
    "word_ini_dict = {}\n",
    "for i, word in enumerate(words):\n",
    "    adjust = i+1 not in pick_1_idxs\n",
    "    word_ini_dict[i+1] = word[:1+int(adjust)]\n",
    "word_ini_dict\n",
    "# 12: Might -> Mi が惜しい。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もし一行で書くなら、、、（わかりやすくインデントつけたけど）\n",
    "{\n",
    "    i+1 : word[:1+int(i+1 not in [1,5,6,7,8,9,15,16,19])] \n",
    "        for i, word in enumerate(\n",
    "            re.sub(\"[,.]\", \"\", sentence).split(\" \")\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. n-gram \n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am an NLPer\"\n",
    "\n",
    "def get_n_gram(sentence:str, target=\"word\", n=2) -> list:\n",
    "    if target==\"word\":\n",
    "        targets = sentence.split(\" \")\n",
    "    elif target==\"letter\":\n",
    "        targets = [l for l in sentence.replace(\" \", \"\")]\n",
    "    else:\n",
    "        raise KeyError(f\"Invalid target key '{target}' was input. Target key should be 'word' or 'letter'.\")\n",
    "    n_grams = []\n",
    "    for i in range(0, len(targets)-(n-1)):\n",
    "        #print(f\"n={n}; i = {i}, then targets[i:i+n] = {targets[i:i+n]}\")\n",
    "        n_grams.append(\n",
    "            \"\".join(targets[i:i+n])\n",
    "        )\n",
    "    else:\n",
    "        return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_keys = [\"word\", \"letter\"]#, \"phrase\"]\n",
    "n = 2\n",
    "for target in target_keys:\n",
    "    print(f\"target_key is {target: >6}, then n_gram(n={n}) is {get_n_gram(sentence, target=target, n=n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart ans?? from u++ -> https://upura.hatenablog.com/entry/2020/04/14/032840\n",
    "def n_gram(target, n):\n",
    "    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n",
    "    # スペースも無視していいのか（一文字扱いしている）\n",
    "\n",
    "text = \"I am an NLPer\"\n",
    "for i in range(1, 4):\n",
    "    print(\"#\"*5, \" n = \", i, \" \", \"#\"*5)\n",
    "    print(n_gram(text, i))\n",
    "    print(n_gram(text.split(\" \"), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. 集合 \n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = \"paraparaparadise\"\n",
    "sent_2 = \"paragraph\"\n",
    "X = get_n_gram(sent_1, target=\"letter\", n=2)\n",
    "Y = get_n_gram(sent_2, target=\"letter\", n=2)\n",
    "print(f\"bi-gram of '{sent_1}' =: X; {X}\")\n",
    "print(f\"bi-gram of '{sent_2}' =: Y; {Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union of X and Y\n",
    "union = set(np.array(X+Y))\n",
    "# product set of X and Y\n",
    "prod  = set([x_ for x_ in X if x_ in Y])\n",
    "# difference set of X and Y\n",
    "diff  = set([x_ for x_ in X if x_ not in Y]+[y_ for y_ in Y if y_ not in X])\n",
    "# \"se\" is in X and Y as bi-gram??\n",
    "se_in_X = \"se\" in X\n",
    "se_in_Y = \"se\" in Y\n",
    "se_in_XandY = se_in_X&se_in_Y\n",
    "\n",
    "print(f\"     union set of X and Y: {union}\")\n",
    "print(f\"   product set of X and Y: {prod}\")\n",
    "print(f\"difference set of X and Y: {diff}\")\n",
    "print(f\"'se' is in X and Y??: in X; {se_in_X}, in Y; {se_in_Y}, totally {se_in_XandY}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logical operator from u++ -> https://upura.hatenablog.com/entry/2020/04/14/033506\n",
    "print(f\"     union set of X and Y: {set(X) | set(Y)}\")\n",
    "print(f\"   product set of X and Y: {set(X) & set(Y)}\")\n",
    "print(f\"difference set of X and Y: {set(X) - set(Y)}\") # ここの出力が違う？？\n",
    "print(f\"'se' is in X and Y??: in X; {'se' in set(X)}, in Y; {'se' in set(X)}, totally {'se' in (set(X) & set(Y))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. テンプレートによる文生成 \n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_template(x, y, z):\n",
    "    return f\"{str(x)}時の{str(y)}は{str(z)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_template(x=12, y=\"気温\", z=22.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. 暗号文 \n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "\n",
    "- 英小文字ならば(219 - 文字コード)の文字に置換\n",
    "- その他の文字はそのまま出力\n",
    "- この関数を用い，英語のメッセージを暗号化・復号化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 文字の文字コードを取得 ... `ord()`\n",
    "- 文字コードの文字を取得 ... `chr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cipher(letters):\n",
    "    return \"\".join([\n",
    "        letter if not letter.islower() else chr(219-ord(letter)) for letter in letters\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"We have a Gift 4 You.\"\n",
    "encod  = cipher(origin)\n",
    "decod  = cipher(encod)\n",
    "print(f\"original sentence: {origin}\")\n",
    "print(f\" encoded sentence: {encod}\")\n",
    "print(f\" decoded sentence: {decod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cipher(text):\n",
    "    text = [chr(219 - ord(w)) if 97 <= ord(w) <= 122 else w for w in text]\n",
    "    return ''.join(text)\n",
    "\n",
    "\n",
    "text = 'this is a message.'\n",
    "ans = cipher(text)\n",
    "print(ans)\n",
    "ans = cipher(ans)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Typoglycemia \n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def rearrange_words(sentence, threshold_len=4, with_space=False) -> str:\n",
    "    words = re.sub(r\"[ ]+\", r\" \", re.sub(r\"([,.:])\", r\" \\1\", sentence)).split(\" \")\n",
    "    # aaa, b: cc. -> aaa , b : cc . -> [\"aaa\", \",\", \"b\", \":\", \"cc\", \".\"]\n",
    "    # 最初から a , b って感じに,.:の前後にスペースが入っていたなら、re.sub(r\"([,.:])\", r\" \\1\", sentence) だけだと\n",
    "    # 空白のみもsplitで得てしまうので、空白の重複を消すための re.sub(r\"[ ]+\", r\" \", を頭に設置\n",
    "    re_words = []\n",
    "    for word in words:\n",
    "        re_word = \"\"\n",
    "        if len(word)<=threshold_len:\n",
    "            re_word = word\n",
    "        else:\n",
    "            re_word = word[0]\n",
    "            re_word += \"\".join(random.sample(word[1:-1], len(word[1:-1])))\n",
    "            re_word += word[-1]\n",
    "        re_words.append(re_word)\n",
    "    else:\n",
    "        if with_space: # ,.:の前後にもスペースがついた状態で返す\n",
    "            return \" \".join(re_words)\n",
    "        else:\n",
    "            return re.sub(r' ([,.:])', r'\\1', \" \".join(re_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
    "print(\"include blank in sentence around ',.:', \\n\",\n",
    "    rearrange_words(sentence, \n",
    "                    threshold_len=4,\n",
    "                    with_space=True)\n",
    ")\n",
    "print(\"#=-=\"*10)\n",
    "print(\"ignore blank in sentence around ',.:', \\n\",\n",
    "    rearrange_words(sentence, \n",
    "                    threshold_len=4,\n",
    "                    with_space=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from u++ -> https://upura.hatenablog.com/entry/2020/04/14/035314\n",
    "def shuffleWord(word):\n",
    "    if len(word) <= 4:\n",
    "        return word\n",
    "    else:\n",
    "        start = word[0]\n",
    "        end = word[-1]\n",
    "        others = random.sample(list(word[1:-1]), len(word[1:-1]))\n",
    "        return ''.join([start] + others + [end])\n",
    "\n",
    "\n",
    "text = 'I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'\n",
    "ans = [shuffleWord(w) for w in text.split()]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第２章 UNIXコマンド\n",
    "\n",
    "[popular-names.txt](https://nlp100.github.io/data/popular-names.txt)は，アメリカで生まれた赤ちゃんの「名前」「性別」「人数」「年」をタブ区切り形式で格納したファイルである．以下の処理を行うプログラムを作成し，[popular-names.txt](https://nlp100.github.io/data/popular-names.txt)を入力ファイルとして実行せよ．さらに，同様の処理をUNIXコマンドでも実行し，プログラムの実行結果を確認せよ．\n",
    "\n",
    "----\n",
    "\n",
    "※参考unixコマンド一覧：http://www.ritsumei.ac.jp/~tomori/unix.html  \n",
    "※Linuxコマンド：https://qiita.com/mtakehara21/items/43785df359eb276adee2  \n",
    "unixコマンドだけで対応しきれない？Linuxコマンドも使用可？linuxコマンドもunixコマンドって捉えていいのか？  \n",
    "でもLinuxコマンドだとWindows OS上で動作しないとかっていう問題も起こりうるよな、、？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp100.github.io/data/popular-names.txt -P ./section02/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat section02/popular-names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 行数のカウント \n",
    "行数をカウントせよ．確認にはwcコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_10():\n",
    "    print(np.loadtxt(\"section02/popular-names.txt\", dtype=\"str\").shape[0])\n",
    "knock_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wc` command returns -> numbers of lines, words, and letters, respectevily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc popular-names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas を使ってもおｋ\n",
    "import pandas as pd\n",
    "pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", header=None).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. タブをスペースに置換 \n",
    "タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://qiita.com/shuntaro_tamura/items/e4e942e7186934fae5e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def knock_11():\n",
    "    with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.replace(\"\\t\", \" \").strip() for line in lines]\n",
    "knock_11()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas ver\n",
    "pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", header=None\n",
    "           ).to_csv(\"section02/popular-names-space-delimiter.txt\", sep=\" \", index=False, header=None)\n",
    "!cat section02/popular-names.txt | head -n3\n",
    "!cat section02/popular-names-space-delimiter.txt | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sed -e \"s/\\t/ /g\" popular-names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 1列目をcol1.txtに，2列目をcol2.txtに保存 \n",
    "各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_12():\n",
    "    with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    with open(\"section02/col1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(\n",
    "            [line.split(\"\\t\",1)[0]+\"\\n\" for line in lines]\n",
    "        )\n",
    "    with open(\"section02/col2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(\n",
    "            [line.split(\"\\t\",2)[1]+\"\\n\" for line in lines]\n",
    "        )\n",
    "    \n",
    "knock_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas ver\n",
    "def knock_12_pd():\n",
    "    df = pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", header=None)\n",
    "    df[0].to_csv(\"section02/col1.txt\", index=False, header=None)\n",
    "    df[1].to_csv(\"section02/col2.txt\", index=False, header=None)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"section02/col1.txt\", \"r\") as f:\n",
    "    print(f.readline().strip())\n",
    "    #print(f.readlines())\n",
    "with open(\"section02/col2.txt\", \"r\") as f:\n",
    "    print(f.readline().strip())\n",
    "    #print(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut section02/popular-names.txt -f 1 > col1.txt\n",
    "!cut section02/popular-names.txt -f 2 > col2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. col1.txtとcol2.txtをマージ \n",
    "12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def knock_13():\n",
    "    col1 = np.loadtxt(\"section02/col1.txt\", dtype=str)\n",
    "    col2 = np.loadtxt(\"section02/col2.txt\", dtype=str)\n",
    "    merged = np.vstack([col1, col2]).T\n",
    "    print(merged)\n",
    "    print(merged.shape)\n",
    "    #np.savetxt(\"section02/merge_col1_2.txt\", merged, delimiter=\"\\t\")\n",
    "knock_13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas ver\n",
    "def knock_13_pd():\n",
    "    col1 = pd.read_csv(\"section02/col1.txt\", header=None)\n",
    "    col2 = pd.read_csv(\"section02/col2.txt\", header=None)\n",
    "    pd.concat([col1, col2], axis=1).to_csv(\"section02/marge_col1_2.txt\", sep=\"\\t\", index=False, header=None)\n",
    "    del col1, col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref; https://www.atmarkit.co.jp/ait/articles/1704/07/news018.html\n",
    "!paste -d \"\\t\" section02/col[1-2].txt > section02/marge_col1_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 先頭からN行を出力 \n",
    "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_14(N: int = None):\n",
    "    if N is None:\n",
    "        N = int(input(\"plz input N (number of read lines)\"))\n",
    "    with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "#        for n_ in range(N):\n",
    "#            print(f.readline().strip())\n",
    "        for i, line in enumerate(f):\n",
    "            if not i==N: print(line.strip())\n",
    "            else:        break\n",
    "knock_14()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from u++ -> https://upura.hatenablog.com/entry/2020/04/14/174852\n",
    "# コマンドライン引数は、`sys.argv`に格納される\n",
    "# e.g. python hoge.py fuga -> sys.argv = [hoge.py, fuga]\n",
    "# argparser を使用するのもアリ\n",
    "import sys\n",
    "def knock_14_pd():\n",
    "    assert len(sys.argv)>1, 'plz set n (e.g. \"python section02_14.py 7\")'\n",
    "    assert isinstance(sys.argv[1], int), f\"input argment as n is not integer (sys.argv[1]={sys.argv[1]}).\"\n",
    "    n = int(sys.argv[1])\n",
    "    pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", dtype=object, header=None).head(n)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    knock_14_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter上だとreadに対して入力ができなかった、、、\n",
    "#!echo -n N:\n",
    "#!read num_lines\n",
    "!head -n 7 section02/popular-names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 末尾のN行を出力 \n",
    "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case use pandas\n",
    "def knock_15(N: int = None):\n",
    "    if N is None:\n",
    "        N = int(input(\"plz input N (number of read lines)\"))\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", dtype=object, header=None)\n",
    "    return df.tail(N).values\n",
    "knock_15()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from u++ -> https://upura.hatenablog.com/entry/2020/04/14/175043\n",
    "# コマンドライン引数は、`sys.argv`に格納される\n",
    "# e.g. python hoge.py fuga -> sys.argv = [hoge.py, fuga]\n",
    "# argparser を使用するのもアリ\n",
    "import sys\n",
    "def knock_15_pd():\n",
    "    assert len(sys.argv)>1, 'plz set n (e.g. \"python section02_14.py 7\")'\n",
    "    assert isinstance(sys.argv[1], int), f\"input argment as n is not integer (sys.argv[1]={sys.argv[1]}).\"\n",
    "    n = int(sys.argv[1])\n",
    "    pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", dtype=object, header=None).tail(n)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    knock_15_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case use only default libraries\n",
    "def knock_15(N: int = None):\n",
    "    if N is None:\n",
    "        N = int(input(\"plz input N (number of read lines)\"))\n",
    "    with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-1*N:]:\n",
    "            print(line.strip())\n",
    "knock_15()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 7 section02/popular-names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ファイルをN分割する \n",
    "自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"section02/xaa_knock16_split_by_line.txt\", \"r\") as f:\n",
    "    f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_16(N: int = None, splitby: str = \"line\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    splitby : str, default \"line\"\n",
    "        if \"line\", then split file into files by each N line (create total_line/N files).\n",
    "        if \"block\", then split file into N files.\n",
    "    \"\"\"\n",
    "    if N is None:\n",
    "        N = int(input(\"plz input N (number of read lines)\"))\n",
    "    if splitby==\"line\":\n",
    "        with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "            lines = []\n",
    "            for i, line in enumerate(f):\n",
    "                lines.append(line)\n",
    "                if i%N==N-1:\n",
    "                    with open(f\"section02/knock16_{i//N}_split_by_line.txt\", \"w\") as fw:\n",
    "                        fw.writelines(lines)\n",
    "                        lines = []                    \n",
    "            else:\n",
    "                if len(lines):\n",
    "                    with open(f\"section02/knock16_{i//N}_split_by_line.txt\", \"w\") as fw:\n",
    "                        fw.writelines(lines)\n",
    "    elif splitby==\"block\":\n",
    "        with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        len_f = len(lines)\n",
    "        N_lines = len_f//N\n",
    "        idx = 0\n",
    "        for i in range(N + (1 if len_f%N else 0)):\n",
    "            with open(f\"section02/knock16_{i}_split_in_block.txt\", \"w\") as fw:\n",
    "                fw.writelines(lines[idx:min(idx+N_lines, len_f)])\n",
    "            idx += N_lines\n",
    "\n",
    "knock_16(splitby=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas ver\n",
    "def knock_16_pd():\n",
    "    assert len(sys.argv)>1, 'plz set n (e.g. \"python section02_16.py 7\")'\n",
    "    assert isinstance(sys.argv[1], int), f\"input argment as n is not integer (sys.argv[1]={sys.argv[1]}).\"\n",
    "    n = int(sys.argv[1])\n",
    "    df = pd.read_csv(\"section02/popular-names.txt\", delimiter=\"\\t\", dtype=object, header=None)\n",
    "    nrow = -(-len(df) // n)\n",
    "    for i in range(n):\n",
    "        df.iloc[nrow * i : nrow * (i + 1)].to_csv(f\"section02/knock16_{i}_pandas-ver_popular-names.txt\", \n",
    "                                                  delimiter=\"\\t\", index=False, header=None)\n",
    "if __name__==\"__main__\":\n",
    "    knock_16_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.atmarkit.co.jp/ait/articles/1711/24/news016.html\n",
    "# 問題文が難しい、N行ごとに1つのファイルを生成するのか、N個のファイルに分割するのか、、\n",
    "\n",
    "# N行ごとなら、\n",
    "!split -l 7 --additional-suffix \"section02/_knock16_split_by_line.txt\" section02/popular-names.txt\n",
    "\n",
    "# N分割するなら、、\n",
    "#!split -n 7 --additional-suffix \"_knock16_split_in_block.txt\" section02/popular-names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 削除してディレクトリ整理用\n",
    "!find ./section02/ -name \"knock16_*_split_by_line.txt\"  | xargs rm\n",
    "#!find ./section02/ -name \"knock16_*_split_in_block.txt\" | xargs rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. １列目の文字列の異なり \n",
    "1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはcut, sort, uniqコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def knock_17():\n",
    "    with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "        all_1st_strings = []\n",
    "        for line in f:\n",
    "            all_1st_strings.append(line.split(\"\\t\")[0])\n",
    "    return set(all_1st_strings)\n",
    "knock_17()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas ver\n",
    "def knock_17_pd():\n",
    "    print(\n",
    "        pd.read_csv(\"section02/popular-names.txt\", sep=\"\\t\", header=None).iloc[:,0].unique()\n",
    "    )\n",
    "knock_17_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut section02/popular-names.txt -f 1 | sort | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 各行を3コラム目の数値の降順にソート \n",
    "各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 逆順って何、降順？？？\n",
    "def knock_18():\n",
    "    line_0, line_1, line_2, line_3 = [], [], [], []\n",
    "    with open(\"section02/popular-names.txt\", \"r\") as f:\n",
    "        lines = [line.strip().split(\"\\t\") for line in f]\n",
    "    return sorted(lines, key=lambda line: int(line[2]), reverse=True)\n",
    "knock_18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_18_pd():\n",
    "    df = pd.read_csv(\"section02/popular-names.txt\", sep=\"\\t\", header=None)\n",
    "    print(df.sort_values(2, ascending=False))\n",
    "\n",
    "knock_18_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref -> https://eng-entrance.com/linux-command-sort\n",
    "# 「-t \"\\t\"」->「-t$\"\\t\"」で対応できるとのこと（ref->https://stackoverflow.com/questions/1037365/sorting-a-tab-delimited-file）\n",
    "#!sort -t$'\\t' -k3 -r popular-names.txt\n",
    "# 上記コマンドを実行すると、エラーを吐いた「sort: multi-character tab ‘$\\\\t’」\n",
    "# printfで出力した結果を用いることで対応 (ref ->https://qiita.com/miminashi/items/a0d22ada995b8bbdff16)\n",
    "!sort -t \"`printf '\\t'`\" -k3 -n -r section02/popular-names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる \n",
    "各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def knock_19():\n",
    "    with open(\"popular-names.txt\", \"r\") as f:\n",
    "        col1 = [line.split(\"\\t\")[0] for line in f]\n",
    "    col1_unis = set(col1)\n",
    "    col1_freq = {}\n",
    "    for col1_uni in col1_unis:\n",
    "        col1_freq[col1_uni] = sum([c==col1_uni for c in col1])\n",
    "    #return [key_and_val[0] for key_and_val in sorted(col1_freq.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return sorted(col1_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "knock_19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_19_pd():\n",
    "    df = pd.read_csv(\"section02/popular-names.txt\", sep=\"\\t\", header=None)\n",
    "    print(df[0].value_counts())\n",
    "\n",
    "knock_19_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut popular-names.txt -f 1 | sort | uniq -c | sort -n -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第３章 正規表現\n",
    "\n",
    "Wikipediaの記事を以下のフォーマットで書き出したファイル[jawiki-country.json.gz](https://nlp100.github.io/data/jawiki-country.json.gz)がある．\n",
    "\n",
    "- 1行に1記事の情報がJSON形式で格納される\n",
    "- 各行には記事名が”title”キーに，記事本文が”text”キーの辞書オブジェクトに格納され，そのオブジェクトがJSON形式で書き出される\n",
    "- ファイル全体はgzipで圧縮される  \n",
    "\n",
    "以下の処理を行うプログラムを作成せよ．\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp100.github.io/data/jawiki-country.json.gz -P ./section03/\n",
    "!gunzip section03/jawiki-country.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. JSONデータの読み込み \n",
    "Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．問題21-29では，ここで抽出した記事本文に対して実行せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    title                                               text\n0    エジプト  {{otheruses|主に現代のエジプト・アラブ共和国|古代|古代エジプト}}\\n{{基礎...\n1  オーストリア  {{基礎情報 国\\n|略名 = オーストリア\\n|日本語国名 = オーストリア共和国\\n|公...\n2  インドネシア  {{基礎情報 国\\n| 略名 =インドネシア\\n| 日本語国名 =インドネシア共和国\\n| ...\n3     イラク  {{複数の問題\\n| 参照方法 = 2011年8月\\n| 独自研究 = 2012年10月\\n...\n4     イラン  {{半保護}}\\n{{未検証|date=2010年3月}}\\n{{基礎情報 国\\n | 略名...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>エジプト</td>\n      <td>{{otheruses|主に現代のエジプト・アラブ共和国|古代|古代エジプト}}\\n{{基礎...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>オーストリア</td>\n      <td>{{基礎情報 国\\n|略名 = オーストリア\\n|日本語国名 = オーストリア共和国\\n|公...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>インドネシア</td>\n      <td>{{基礎情報 国\\n| 略名 =インドネシア\\n| 日本語国名 =インドネシア共和国\\n| ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>イラク</td>\n      <td>{{複数の問題\\n| 参照方法 = 2011年8月\\n| 独自研究 = 2012年10月\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>イラン</td>\n      <td>{{半保護}}\\n{{未検証|date=2010年3月}}\\n{{基礎情報 国\\n | 略名...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "# linesz=False(default) -> raise ValueError: Trailing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ",\n '',\n '「イギリス民族」という民族は存在しない。主な民族はイングランドを中心に居住する[[ゲルマン人|ゲルマン民族]]系のイングランド人（[[アングロ・サクソン人]]）、[[ケルト人|ケルト]]系のスコットランド人、アイルランド人、ウェールズ人だが、旧植民地出身のインド系（[[印僑]]）、[[アフリカ系]]、カリブ系、[[アラブ系]]や[[華僑]]なども多く住む[[多民族国家]]である。',\n '',\n 'イギリスの国籍法では、旧植民地関連の者も含め、自国民を次の六つの区分に分けている。',\n '*GBR:British Citizen - イギリス市民',\n '*:本国人',\n '*BOTC:[[:en:British Overseas Territories citizen|British Overseas Territories citizen]] - [[イギリス海外領土市民]]',\n '*:イギリスの海外領土出身者',\n '*BOC:[[:en:British Overseas Citizen|British Overseas Citizen]] - [[イギリス海外市民]]',\n '*:ギリシャ西岸の諸島・インド・パキスタン・マレーシアなどの旧植民地出身者のうち特殊な歴史的経緯のある者',\n '*GBS:[[:en:British Subject|British Subject]] - [[イギリス臣民]]',\n '*:アイルランド（北部以外）・ジブラルタルなどイギリス海外領土市民やイギリス海外市民とは別の経緯のある地域の住民で一定要件に該当する者',\n '*BNO:[[:en:British National (Overseas)|British National (Overseas)]] - [[イギリス国民（海外）]]※「BN(O)」とも書く。',\n '*:英国国籍で、香港の[[永住権|住民権]]も持つ人。',\n '*BPP:[[:en:British Protected Person|British Protected Person]] - [[イギリス保護民]]',\n '',\n 'いずれの身分に属するかによって、国内での様々な取扱いで差異を生ずることがあるほか、パスポートにその区分が明示されるため、海外渡航の際も相手国により取扱いが異なることがある。例えば、日本に入国する場合、British citizen（本国人）とBritish National (Overseas)（英国籍香港人）は短期訪問目的なら[[査証]]（ビザ）不要となるが、残りの四つは数日の[[観光]]訪日であってもビザが必要となる。',\n '',\n '===言語===',\n '{{main|{{仮リンク|イギリスの言語|en|Languages of the United Kingdom}}}}',\n '[[ファイル:Anglospeak.svg|thumb|250px|世界の[[英語圏]]地域。濃い青色は英語が[[公用語]]または事実上の公用語になっている地域。薄い青色は英語が公用語であるが主要な言語ではない地域。]]',\n '事実上の公用語は英語（イギリス英語）でありもっとも広く使用されているが、イングランドの主に[[コーンウォール]]でコーンウォール語、ウェールズの主に北部と中部でウェールズ語、スコットランドの主に[[ローランド地方]]でスコットランド語、ヘブリディーズ諸島の一部でスコットランド・ゲール語、北アイルランドの一部で[[:en:Ulster Scots dialects|アルスター・スコットランド語]]とアイルランド語が話されており、それぞれの構成国で公用語になっている。',\n '',\n '特に、ウェールズでは1993年にウェールズ語が公用語になり、英語と同等の法的な地位を得た。2001年現在、ウェールズ人口の約20%がウェールズ語を使用し、その割合は僅かではあるが増加傾向にある。公文書や道路標識などはすべてウェールズ語と英語とで併記される。また、16歳までの義務教育においてウェールズ語は必修科目であり、ウェールズ語を主要な教育言語として使用し、英語は第二言語として扱う学校も多く存在する。',\n '',\n '===宗教===',\n '{{See also|イギリスの宗教}}',\n '10年に一度行われるイギリス政府の国勢調査によれば、2001年、[[キリスト教徒]]が71.7%、[[イスラム教徒]]が3.0%、[[ヒンドゥー教]]徒が1.0%。',\n '2011年、キリスト教徒59.3%、イスラム教徒4.8%、ヒンドゥー教徒が1.5%<ref>{{Cite report |publisher=Office for National Statistics |title=Religion in England and Wales 2011 |date=2012-12-11 |url=https://www.ons.gov.uk/peoplepopulationandcommunity/culturalidentity/religion/articles/religioninenglandandwales2011/2012-12-11 }}</ref>。',\n '',\n 'キリスト教の内訳は、[[英国国教会]]が62%、[[カトリック]]が13%、[[長老派]]が6%、[[メソジスト]]が3%程度と推定されている<ref>『The Changing Religious Landscape of Europe』 Hans Knippenberg</ref>。',\n '',\n '===婚姻===',\n '婚姻の際には、夫婦同姓・複合姓・[[夫婦別姓]]のいずれも選択可能である。また[[同性結婚]]も可能である<ref>「英国・イングランドとウェールズ、同性婚を初の合法化」朝日新聞、2014年{{0}}3月29日</ref>。また、在日英国大使館においても、同性結婚登録を行うことが可能である<ref>「在日本英国大使館・領事館で同性婚登録が可能に」 週刊金曜日 2014年{{0}}6月13日</ref><ref>https://www.gov.uk/government/news/introduction-of-same-sex-marriage-at-british-consulates-overseas.ja</ref>。',\n '',\n '===移住===',\n '{{main|{{仮リンク|現代イギリスにおける移民|en|Modern immigration to the United Kingdom}}|{{仮リンク|イギリスにおける外国人出生者|en|Foreign-born population of the United Kingdom}}}}',\n '{{節スタブ}}',\n '===教育===',\n '{{main|イギリスの教育}}',\n 'イギリスの学校教育は地域や公立・私立の別により異なるが、5歳より小学校教育が開始される。',\n '',\n '===医療===',\n '{{Main|イギリスの医療}}',\n \"[[ファイル:Royal Aberdeen Children's Hospital.jpg|thumb|right|The Royal Aberdeen Children's Hospital。NHSスコットランドの小児病院]]\",\n 'イギリスの医療は各地域それぞれの[[地方分権]]型であり、公的医療とプライベート診療が存在する。公的医療は[[国民保健サービス]]（NHS）によりすべてのイギリス人に提供され、医学的必要性が認められる治療は大部分は自己負担なしであり、費用は一般税収を原資としている（[[公費負担医療]]）。NHSにはイギリス国家予算の25.2%が投じられている<ref name=\"ohe\">{{Cite report |publisher=Office of Health Economics |title=OHE Guide to UK Health and Health Care Statistics |date=2013-08 |url=https://www.ohe.org/publications/ohe-guide-uk-health-and-health-care-statistics }}</ref>。',\n '',\n \"国全体にかかわる規制は、{{仮リンク|総合医療評議会|en|General Medical Council}}や{{仮リンク|看護助産評議会|en|Nursing and Midwifery Council}}や、また[[ロイヤル・カレッジ]]などの外部機関が行っている。しかし政策や現業の責務は、各地方行政区である4つの女王陛下の政府、北アイルランド政府、スコットランド政府、ウェールズ政府がそれぞれになっている。それぞれの運営するNHSは、各々の政策や優先度を持ち、施政に違いをもたらしている<ref>{{cite news |url=http://news.bbc.co.uk/1/hi/health/7586147.stm |title='Huge contrasts' in devolved NHS |work=BBC News |date=28 August 2008}}</ref><ref>{{cite news |url=http://news.bbc.co.uk/1/hi/health/7149423.stm |title=NHS now four different systems |work=BBC News |date=2 January 2008 |first=Nick |last=Triggle}}</ref>。\",\n '',\n '英国はGDPの8.5%を医療に支出しており、これはOECD平均と比べて-0.5%、EU平均と比べて-1%の値であった<ref>{{cite report|title=OECD Health Data 2009&nbsp;– How Does the United Kingdom Compare |publisher=OECD}}</ref>。1979年に保健支出が急増したことにより、その値はEU平均に近くなってきている<ref>{{Cite journal|url=http://www.healthp.org/node/71|title=The NHS from Thatcher to Blair |first=Peter |last=Fisher |work=NHS Consultants Association |publisher=International Association of Health Policy |quote=The Budget ... was even more generous to the NHS than had been expected amounting to an annual rise of 7.4% above the rate of inflation for the next 5 years. This would take us to 9.4% of GDP spent on health ie around EU average.}}</ref>。WHOは2000年に英国の医療制度を欧州で15位、世界で18位と評している<ref name=\"Who2000\">{{Cite book |editor-last=Haden |editor-first=Angela |editor2-last=Campanini |editor2-first=Barbara |title=The world health report 2000&nbsp;– Health systems:improving performance |year=2000 |location=Geneva |publisher=World Health Organisation |url=http://www.who.int/whr/2000/en/whr00_en.pdf |isbn=92-4-156198-X |accessdate=5 July 2011}}</ref><ref>{{Cite journal |url=http://pages.stern.nyu.edu/~wgreene/Statistics/WHO-COMP-Study-30.pdf |title=Measuring overall health system performance for 191 countries |author=[[World Health Organization]] |publisher=New York University |accessdate=5 July 2011}}</ref>。',\n '',\n '==文化==',\n '{{Main|[[:en:Culture of the United Kingdom]]}}',\n '',\n '===食文化===',\n '{{Main|イギリス料理}}',\n '[[フィッシュ・アンド・チップス]]や[[ローストビーフ]]、[[ウナギのゼリー寄せ]]などが有名である。{{節スタブ}}',\n '',\n '===文学===',\n '[[ファイル:CHANDOS3.jpg|thumb|150px|[[ウィリアム・シェイクスピア]]]]',\n '{{Main|イギリス文学}}',\n '多くの傑作を後世に残した[[ウィリアム・シェイクスピア]]は、[[イギリス・ルネサンス演劇]]を代表する空前絶後の詩人、および劇作家と言われる。初期のイギリス文学者としては[[ジェフリー・オブ・モンマス]]や[[ジェフリー・チョーサー]]、[[トマス・マロリー]]が著名。18世紀になると[[サミュエル・リチャードソン]]が登場する。彼の作品には3つの小説の基本条件、すなわち「フィクション性および物語性、人間同士の関係（愛情・結婚など）、個人の性格や心理」といった条件が満たされていたことから、彼は「近代小説の父」と呼ばれている。',\n '',\n '19世紀の初めになると[[ウィリアム・ブレイク]]、[[ウィリアム・ワーズワース]]ら[[ロマン主義]]の詩人が活躍した。19世紀には小説分野において革新が見られ、[[ジェーン・オースティン]]、[[ブロンテ姉妹]]、[[チャールズ・ディケンズ]]、[[トーマス・ハーディ]]らが活躍した。19世紀末には、[[耽美主義]]の[[オスカー・ワイルド]]、現代の[[推理小説]]の生みの親[[アーサー・コナン・ドイル]]が登場。',\n '',\n '20世紀に突入すると、「[[サイエンス・フィクション|SF]]の父」[[ハーバート・ジョージ・ウェルズ]]、[[モダニズム]]を探求した[[デーヴィッド・ハーバート・ローレンス]]、[[ヴァージニア・ウルフ]]、預言者[[ジョージ・オーウェル]]、「ミステリーの女王」[[アガサ・クリスティ]]などが出てくる。そして近年、[[ハリー・ポッターシリーズ]]の[[J・K・ローリング]]がかつての[[J・R・R・トールキン]]のような人気を世界中で湧かせている。',\n '',\n '===哲学===',\n '{{Main|{{仮リンク|イギリスの哲学|en|British philosophy}}}}',\n '{{節スタブ}}',\n '*[[イギリス経験論]]',\n '*[[イギリス理想主義]]',\n '',\n '===音楽===',\n '{{Main|イギリスの音楽}}',\n '<!-- 音楽の欄はジャンルも影響力もバラバラの人名が並んでいるため、出典に基づいた整理が必要 -->',\n '[[クラシック音楽]]における特筆すべきイギリス人作曲家として、「ブリタニア音楽の父」[[ウィリアム・バード]]、[[ヘンリー・パーセル]]、[[エドワード・エルガー]]、[[アーサー・サリヴァン]]、[[レイフ・ヴォーン・ウィリアムズ]]、[[ベンジャミン・ブリテン]]がいる。特に欧州大陸で古典派、ロマン派が隆盛をきわめた18世紀後半から19世紀にかけて有力な作曲家が乏しかった時期もあったが、旺盛な経済力を背景に演奏市場としては隆盛を続け、ロンドンはクラシック音楽の都の一つとして現在残る。ドイツのオーケストラが地方中都市の団体でも四管フル編成を原則としているのに対し、ロンドン5大オーケストラは長年[[BBC交響楽団]]を除き（現在は[[ロンドン交響楽団]]も）総員70名台の中規模編成を貫き、大曲演奏に際してはフリー奏者を臨時補充するなどの形であったにも関わらず、それなりの世界的声価を維持してきた。一時はメンバーの共有も見られ、映画音楽の仕事が多いことが批判されることもあるものの、持ち前の合理主義によって、少なくとも英語圏では随一のクラシック演奏都市であり続けている。オペラはロンドンに[[ロイヤル・オペラ・ハウス|コヴェントガーデン王立歌劇場]]と、[[イングリッシュ・ナショナルオペラ]]を擁し、後者は世界手も珍しい英訳上演主義の団体である。',\n '',\n '====ポピュラー音楽====',\n '[[ファイル:The Fabs.JPG|thumb|200px|[[ビートルズ]]]]',\n '{{Main|ロック (音楽)|{{仮リンク|ブリティッシュロック|en|British rock}}}}',\n '[[ポピュラー音楽]]（特にロックミュージック）において、イギリスは先鋭文化の発信地として世界的に有名である。1960、70年代になると[[ロック (音楽)|ロック]]が誕生し、中でも[[ビートルズ]]や[[ローリング・ストーンズ]]といった[[ロックンロール]]の影響色濃いバンドが、その表現の先駆者として活躍した。やがて[[キング・クリムゾン]]や[[ピンク・フロイド]]などの[[プログレッシブ・ロック]]や、[[クイーン (バンド)|クイーン]]、[[クリーム (バンド)|クリーム]]、[[レッド・ツェッペリン]]、[[ディープ・パープル]]、[[ブラック・サバス]]などの[[R&B]]や[[ハードロック]]がロックの更新に貢献。1970年代後半の[[パンク・ロック]]の勃興においては、アメリカ・ニューヨークからの文化を取り入れ、ロンドンを中心に[[セックス・ピストルズ]]、[[ザ・クラッシュ]]らが国民的なムーブメントを起こす。',\n '',\n 'パンク・ロック以降はインディー・ロックを中心に[[ニュー・ウェイヴ (音楽)|ニュー・ウェイヴ]]などといった新たな潮流が生まれ、[[テクノポップ]]・ドラッグミュージック文化の発達と共に[[ニュー・オーダー]]、[[ザ・ストーン・ローゼズ]]、[[グリッド]]などが、メインストリームでは[[デュラン・デュラン]]、[[デペッシュ・モード]]らの著名なバンドが生まれた。90年代は[[ブリットポップ]]や[[エレクトロニカ]]がイギリスから世界中に広まり人気を博し、[[オアシス (バンド)|オアシス]]、[[ブラー]]、[[レディオヘッド]]、[[プロディジー]]、[[マッシヴ・アタック]]などは特に目覚ましい。[[シューゲイザー]]、[[トリップホップ]]、[[ビッグビート]]などといった多くの革新的音楽ジャンルも登場した。近年では[[エイミー・ワインハウス]]、[[マクフライ]]、[[コールドプレイ]]、[[スパイス・ガールズ]]らがポップシーンに名を馳せた。',\n '',\n 'イギリスではロックやポップなどのポピュラー音楽が、国内だけでなく世界へ大きな市場を持つ主要な[[外貨]]獲得興業となっており、トニー・ブレア政権下などではクール・ブリタニアでロックミュージックに対する国策支援などが行われたりなど、その重要度は高い。アメリカ合衆国と共にカルチャーの本山として世界的な影響力を保ち続け、他国のポピュラー音楽産業の潮流への先駆性は、近年もいささかも揺るがない。',\n '',\n '===映画===',\n '{{Main|イギリスの映画}}',\n '{{節スタブ}}',\n '',\n '===コメディ===',\n 'イギリス人はユーモアのセンスが高いと言われている。また、コメディアンの多くは高学歴である。',\n '*[[ローワン・アトキンソン]]',\n '*[[チャールズ・チャップリン]]',\n '*[[ピーター・セラーズ]]',\n '*[[モンティ・パイソン]]',\n '*[[リック・ウェイクマン]] （但し、本職は[[ミュージシャン]]）',\n '',\n '===国花===',\n '[[国花]]はそれぞれの地域が持っている。',\n '*イングランドは[[バラ]]',\n '*ウェールズは[[ラッパスイセン]]（[[スイセン]]の1種）。[[リーキ]]もより歴史のあるシンボルだが、リーキは花ではない。',\n '*北アイルランドは[[シャムロック]]',\n '*スコットランドは[[アザミ]]',\n '',\n '===世界遺産===',\n 'イギリス国内には、[[国際連合教育科学文化機関|ユネスコ]]の[[世界遺産]]リストに登録された文化遺産が21件、自然遺産が5件ある。詳細は、[[イギリスの世界遺産]]を参照。',\n '<gallery>',\n 'PalaceOfWestminsterAtNight.jpg|ウェストミンスター宮殿',\n 'Westminster Abbey - West Door.jpg|[[ウェストミンスター寺院]]',\n 'Edinburgh Cockburn St dsc06789.jpg|[[エディンバラ旧市街|エディンバラの旧市街]]・[[エディンバラ新市街|新市街]]',\n 'Canterbury Cathedral - Portal Nave Cross-spire.jpeg|[[カンタベリー大聖堂]]',\n 'Kew Gardens Palm House, London - July 2009.jpg|[[キューガーデン|キュー王立植物園]]',\n '2005-06-27 - United Kingdom - England - London - Greenwich.jpg|[[グリニッジ|マリタイム・グリニッジ]]',\n 'Stonehenge2007 07 30.jpg|[[ストーンヘンジ]]',\n 'Yard2.jpg|[[ダラム城]]',\n 'Durham Kathedrale Nahaufnahme.jpg|[[ダラム大聖堂]]',\n 'Roman Baths in Bath Spa, England - July 2006.jpg|[[バース市街]]',\n 'Fountains Abbey view02 2005-08-27.jpg|[[ファウンテンズ修道院]]跡を含む[[スタッドリー王立公園]]',\n 'Blenheim Palace IMG 3673.JPG|[[ブレナム宮殿]]',\n 'Liverpool Pier Head by night.jpg|[[海商都市リヴァプール]]',\n \"Hadrian's Wall view near Greenhead.jpg|[[ローマ帝国の国境線]] ([[ハドリアヌスの長城]])\",\n 'London Tower (1).JPG|[[ロンドン塔]]',\n '</gallery>',\n '',\n '===祝祭日===',\n '祝祭日は、イングランド、ウェールズ、スコットランド、北アイルランドの各政府により異なる場合がある。銀行など多くの企業が休みとなることから、国民の祝祭日をバンク・ホリデー({{interlang|en|Bank holiday}})（銀行休業日）と呼ぶ。',\n '{|class=\"wikitable\"',\n '!日付!!日本語表記!!現地語表記!!備考',\n '|-',\n \"|1月{{0}}1日||[[元日]]||{{lang|en|New Year's Day}}||\",\n '|-',\n '|1月{{0}}2日||元日翌日||-||スコットランドのみ',\n '|-',\n \"|3月17日||[[聖パトリックの祝日|聖パトリックの日]]||{{lang|en|St. Patrick's Day}}||北アイルランドのみ\",\n '|-',\n '|3月 - 4月||[[聖金曜日]]||{{lang|en|Good Friday}}||移動祝日',\n '|-',\n '|3月 - 4月||[[復活祭]]月曜日||{{lang|en|Easter Monday}}||移動祝日',\n '|-',\n '|5月第1月曜日||[[五月祭]]||{{lang|en|Early May Bank Holiday}}||移動祝日',\n '|-',\n '|5月最終月曜日||五月祭終り||{{lang|en|Spring Bank Holiday}}||移動祝日',\n '|-',\n \"|7月12日||[[ボイン川の戦い]]記念日||{{lang|en|Battle of the Boyne (Orangemen's Day)}}||北アイルランドのみ\",\n '|-',\n '|8月第1月曜日||夏季銀行休業日||{{lang|en|Summer Bank Holiday}}||移動祝日、スコットランドのみ',\n '|-',\n '|8月最終月曜日||夏季銀行休業日||{{lang|en|Summer Bank Holiday}}||移動祝日、スコットランドを除く',\n '|-',\n '|12月25日||[[クリスマス]]||{{lang|en|Christmas Day}}||',\n '|-',\n '|12月26日||[[ボクシング・デー]]||{{lang|en|Boxing Day}}||',\n '|}',\n '*聖金曜日を除く移動祝日は原則的に月曜日に設定されている。',\n '*ボクシング・デー後の2日も銀行休業日であったが2005年を最後に廃止されている。',\n '',\n '===スポーツ===',\n '{{main|{{仮リンク|イギリスのスポーツ|en|Sport in the United Kingdom}}}}',\n '[[ファイル:Wembley Stadium, illuminated.jpg|thumb|220px|[[ウェンブリー・スタジアム]]]]',\n 'イギリスは[[サッカー]]、[[ラグビー]]、[[クリケット]]、[[ゴルフ]]、[[ボクシング]]など多くの競技が発祥もしくは近代スポーツとして整備された地域であり、国技としても定着している。年間観客動員数は4000万人以上を集めるサッカーが他を大きく凌いでおり、[[競馬]]の600万人、ユニオンラグビーの300万、クリケット200万がそれに続く。',\n '',\n 'このうち団体球技（サッカー、ラグビー、クリケット）は発祥地域の伝統的な配慮から国際競技団体ではイギリス単体ではなく、イングランド、スコットランド、ウェールズ、北アイルランド（ラグビーに関してはアイルランドにまとめている）の4地域それぞれの加盟を認めているが、サッカーが公式なプログラムとして行われている[[近代オリンピック]]では単一国家としての出場が大原則であるため、長年出場していなかった。しかし2012年の開催が内定した[[ロンドンオリンピック (2012年)|ロンドン五輪]]では4協会が一体となった統一イギリス代表としてエントリーした。またイギリスの首都であるロンドンで[[夏季オリンピック]]を行ったのは、1948年以来64年ぶりである。ただし[[野球]]においては早くから[[野球イギリス代表|英国代表]]として、[[欧州野球選手権]]や[[ワールド・ベースボール・クラシック|WBC]]などに統一ナショナルチームを送り出している。',\n '',\n '====サッカー====',\n '{{main|{{仮リンク|イギリスのサッカー|en|Football in England}}}}',\n \"数多くのスポーツを誕生させたイギリスでも取り分け人気なのがサッカーである。イギリスでサッカーは「'''フットボール'''」と呼び、近代的なルールを確立したことから「'''近代サッカーの母国'''」と呼ばれ、それぞれの地域に独自のサッカー協会がある。イギリス国内でそれぞれ独立した形でサッカーリーグを展開しており、中でもイングランドの[[プレミアリーグ]]は世界的に人気である。[[フットボール・アソシエーション|イングランドサッカー協会]] (FA) などを含むイギリス国内の地域協会は全て、[[国際サッカー連盟]] (FIFA) よりも早くに発足しており、FIFA加盟国では唯一特例で国内の地域単位での加盟を認められている(以降、FIFAは海外領土など一定の自治が行われている地域協会を認可している)。その為、FIFAや[[欧州サッカー連盟]]（UEFA）が主宰する各種国際大会（[[FIFAワールドカップ]]・[[UEFA欧州選手権]]・[[UEFAチャンピオンズリーグ]]・[[UEFAカップ]]・[[FIFA U-20ワールドカップ]]や[[UEFA U-21欧州選手権]]などの年代別国際大会）には地域協会単位でのクラブチームやナショナルチームを参加させており、さらには7人いるFIFA副会長の一人はこの英本土4協会から選ばれる、サッカーのルールや重要事項に関しては、FIFAと英本土4協会で構成する[[国際サッカー評議会]]が決定するなど特権的な地位が与えられている。また、サッカー選手や監督がプロ競技における傑出した実績によって一代限りの騎士や勲爵士となることがある（[[デビッド・ベッカム]]、[[スティーヴン・ジェラード]]や[[ボビー・ロブソン]]、[[アレックス・ファーガソン]]など）。\",\n '',\n 'また、サッカーはもともとラグビーと同じく中流階級の師弟が通う[[パブリックスクール]]で近代競技として成立したが、その過程は労働者階級の娯楽として発展していった。ただ、当時のイギリスの継続的な不況からくる労働者階級の人口の割合と、それ以外の階級者も観戦していたということを注意しなければならない。労働者階級がラグビーよりもサッカーを好んでいたとされる理由として、[[フーリガン]]というあまり好ましくない暴力的なファンの存在が挙げられることもある。ただ、相次ぐフーリガン絡みの事件や事故を重く見た政府は1980年代にフーリガン規制法を制定し、スタジアムの大幅な安全基準の見直しなどを行った。各スタジアムの試合運営スタッフがスタジアムの至る所に監視カメラを設置し、特定のサポーター（フーリガン）に対する厳重な監視や入場制限を行っている。そのような取り組みの結果、スタジアムではそれまで頻発していたフーリガン絡みの事件や事故の件数が大幅に減少した。',\n '*2007-2008シーズンにおけるイングランドサッカー入場者数<ref>2008年12月10日付けの日本経済新聞</ref>',\n '**プレミアリーグ 1370万8875人',\n '**[[フットボールリーグ・チャンピオンシップ|チャンピオンシップ]] 939万7036人',\n '**[[フットボールリーグ1]] 441万2023人',\n '**[[フットボールリーグ2]] 239万6278人',\n '**[[FAカップ]] 201万1320人',\n '**[[フットボールリーグカップ|リーグカップ]] 133万2841人',\n '**[[UEFAチャンピオンズ・リーグ|CL]] 122万0127人',\n '**UEFAカップ 46万2002人',\n '**総動員数 3494万人',\n '',\n '====クリケット====',\n 'クリケットは全面芝のフィールドでプレイされ、試合中には[[ティー]]タイムもある。その優雅な雰囲気から、別名「紳士のスポーツ」といわれる。イギリスでは[[上流階級]]がたしなむスポーツとされており、[[イートン]]などの名門校の体育ではクリケットは必修種目とされている。16世紀にイングランド南部で初めてプレーされた。18世紀末までには、イングランドの国民的スポーツへと発展した。大英帝国の拡大によってクリケットは海外でプレーされるようになり、19世紀中頃までには初の国際試合が開催された。イングランドは[[国際クリケット評議会]]（ICC）のフルメンバーである。[[クリケット・ワールドカップ]]は[[FIFAワールドカップ]]と[[夏季オリンピック]]に次いで世界で3番目に視聴者数の多いスポーツイベントであり<ref>[https://www.theguardian.com/sport/blog/2015/feb/12/cricket-world-cup-icc-50-overs More money, more viewers and fewer runs in prospect for intriguing World Cup More money, more viewers and fewer runs in prospect for intriguing World Cup] The Guardian 2019年7月15日閲覧。</ref>、自国開催の[[2019 クリケット・ワールドカップ|2019年]]大会では[[クリケットイングランド代表|イングランド代表]]が初優勝した<ref>[https://www.theguardian.com/sport/live/2019/jul/14/new-zealand-v-england-cricket-world-cup-final-2019-live England beat New Zealand in thrilling Cricket World Cup final – as it happened!] The Guardian 2019年7月18日閲覧。</ref>。女子イングランド代表は[[女子クリケット・ワールドカップ|ワールドカップ]]で4度の優勝経験を誇る。ロンドンにある[[ローズ・クリケット・グラウンド]]はクリケットの聖地と呼ばれ、ワールドカップの決勝戦などが催された。国内リーグは[[カウンティ・チャンピオンシップ]]があり、イングランド所在の17クラブ及びウェールズ所在の1クラブ、合計18クラブにより編成される。',\n '',\n '====競馬====',\n '{{main|イギリスの競馬}}',\n '近代競馬発祥の地でもある。18世紀ゴルフに次いでスポーツ組織として[[ジョッキークラブ]]が組織され、同時期に[[サラブレッド]]も成立した。どちらかと言えば[[平地競走]]よりも[[障害競走]]の方が盛んな国であり、\"Favourite 100 Horses\"（好きな馬100選）では[[アークル]]を初め障害馬が上位を独占した。障害の[[チェルトナムフェスティバル]]や[[グランドナショナルミーティング]]は15～25万人もの観客動員数がある。特に最大の競走であるG3[[グランドナショナル]]の売り上げは700億円近くになり、2007年現在世界で最も馬券を売り上げる競走になっている。平地競走は、[[ダービーステークス|イギリスダービー]]、[[イギリス王室|王室]]開催の[[ロイヤルアスコット開催]]が知られ、こちらも14～25万人の観客を集める。ダービーは、この競走を冠した競走が競馬を行っている国には必ずと言っていい程存在しており世界で最も知られた競走といって良いだろう。エリザベス女王も競馬ファンとして知られており、自身何頭も競走馬を所有している。',\n '',\n 'イギリスでは、日本などと違い競馬など特定の競技だけでなく全てのスポーツがギャンブルの対象となるが、売り上げはやはり競馬とサッカーが多い。競馬は1970年代を頂点に人気を失いつつあったが、後に急速に観客動員数が持ち直す傾向にある。売上高も2兆円を超え、人口当りの売り上げは香港を除けばオーストラリアに次ぐ。しかし、売り上げの多く（2003年で97.1%）が主催者側と関係のない[[ブックメーカー]]に占められるという構造的な課題がある。なお、イギリス人はどんな小さな植民地にも大抵の場合は競馬場を建設したため、独立後も旧イギリス領は競馬が盛んな国が多い。また、[[馬術]]も盛んであり、馬術のバドミントンは3日間で15万人以上の観客動員数がある。',\n '',\n '====モータースポーツ====',\n 'イギリスは、[[モータースポーツ]]発祥の地としても知られる。[[フォーミュラ1]]（F1）では多数のチャンピオンドライバーを生み出している。最近では、2009年世界チャンピオンに[[ジェンソン・バトン]]、そして2008、2014、2015、2017、2018、2019年世界チャンピオンに６度[[ルイス・ハミルトン]]が輝き、あと1回世界チャンピオンになれば、[[ミハエル・シューマッハ]]のもつ７度の記録と並ぶことになる。',\n '',\n '過去には[[チーム・ロータス|ロータス]]や[[ティレル]]、現在も[[マクラーレン]]、[[ウィリアムズF1|ウィリアムズ]]といった数多くの名門レーシングチームが本拠を置き、車両の設計製造において常に最先端を行く。',\n '',\n 'イベントにも歴史があり、1926年に初開催された[[イギリスグランプリ]]は最も古いグランプリレースのひとつである。1950年に始まったF1グランプリはイギリスグランプリを第1戦とした。また[[世界ラリー選手権]]の一戦として組み込まれているラリー・グレート・ブリテン（1933年初開催）も同シリーズの中でもっとも古いイベントの一つである。',\n '',\n '====野球====',\n '{{main|ナショナルリーグ (イギリスの野球)}}',\n '知名度は低いが、1890年に[[ブリティッシュ・ベースボール・リーグ]]という野球リーグが誕生している。[[IBAFワールドカップ]]の[[ジョン・ムーアズ・トロフィー|第1回大会]]では、アメリカ合衆国との二カ国対抗戦という形ではあったが、5回戦制のこの大会を4勝1敗で勝ち、最初の優勝国となっている。2012年{{0}}9月には、[[2013 ワールド・ベースボール・クラシック|第3回WBC]]予選に出場している。',\n '',\n '==== カーリング ====',\n 'あまり知られてはいないが、イギリスはカーリングの強豪国でもある。<ref>「[http://www.curling.or.jp/newinfo/newrankings.html 世界ランク男女]」</ref>',\n '',\n '==== 自転車競技 ====',\n '国内での人気はサッカーなどには劣るが、[[ロードレース (自転車競技)|ロードレース]]や[[トラックレース]]では世界でも[[フランス]]、[[スペイン]]、[[イタリア]]と肩を並べる強豪国である。ロードレースでは2012年に[[ブラッドリー・ウィギンス]]が[[ツール・ド・フランス2012|ツール・ド・フランス]]を英国人として初めて制覇し、[[クリス・フルーム]]が2013年、2015年-2017年と同大会で総合優勝し、また2017年には[[ブエルタ・ア・エスパーニャ2017|ブエルタ・ア・エスパーニャ]]を、2018年には[[ジロ・デ・イタリア2018|ジロ・デ・イタリア]]を制覇し、[[グランツール]]と呼ばれる世界三大大会を年を跨いで連続制覇した史上3人目の選手となるなど近年目覚ましい活躍を見せている。トラックレースでもウィギンスや[[ゲラント・トーマス]]、[[エド・クランシー]]らが[[世界選手権自転車競技大会|世界選手権]]や[[オリンピック]]で数々のメダルを獲得している。',\n '',\n '==脚注==',\n '{{脚注ヘルプ}}',\n '{{Reflist|2}}',\n '',\n '==関連項目==',\n '*[[イギリス関係記事の一覧]]',\n '',\n '==外部リンク==',\n '{{ウィキポータルリンク|イギリス}}',\n '{{Sisterlinks|commons=United Kingdom|commonscat=United Kingdom|s=Category:イギリス|n=Category:イギリス|voy=United Kingdom}}',\n ';本国政府',\n '*[https://www.royal.uk/ 英国王室（The British Monarchy）] {{en icon}}',\n '**{{Facebook|TheBritishMonarchy|The British Monarchy}} {{en icon}}',\n '**{{Twitter|BritishMonarchy|BritishMonarchy}} {{en icon}}',\n '**{{flickr|photos/britishmonarchy/|The British Monarchy}} {{en icon}}',\n '**{{YouTube|user=TheRoyalChannel|The British Monarchy}} {{en icon}}',\n '*[https://www.gov.uk/ 英国政府（GOV.UK）] {{en icon}}',\n \"*[https://www.gov.uk/government/organisations/prime-ministers-office-10-downing-street 英国首相府（Prime Minister's Office, 10 Downing Street）] {{en icon}}\",\n '**{{Facebook|10downingstreet|10 Downing Street}} {{en icon}}',\n '**{{Twitter|@Number10gov|UK Prime Minister}} {{en icon}}',\n '**{{Twitter|@Number10press|No.10 Press Office}} {{en icon}}',\n '**{{flickr|photos/number10gov|Number 10}} {{en icon}}',\n '**{{Pinterest|number10gov|UK Prime Minister}} {{en icon}}',\n '**{{YouTube|user=Number10gov|Number10gov|films and features from Downing Street and the British Prime Minister}} {{en icon}}',\n '**{{YouTube|user=DowningSt|Downing Street|archive footage from Downing Street and past British Prime Ministers}} {{en icon}}',\n '*[https://www.gov.uk/government/world/japan.ja UK and Japan (UK and the world - GOV.UK)] {{ja icon}}{{en icon}}',\n '**[https://www.gov.uk/government/world/organisations/british-embassy-tokyo.ja 駐日英国大使館（GOV.UK）] {{ja icon}}{{en icon}}',\n '***{{Facebook|ukinjapan|British Embassy Tokyo}} {{ja icon}}{{en icon}}※使用言語は個々の投稿による',\n '***{{Twitter|UKinJapan|BritishEmbassy英国大使館}} {{ja icon}}{{en icon}}※使用言語は個々の投稿による',\n '***{{flickr|photos/uk-in-japan|UK in Japan- FCO}} {{en icon}}',\n '***{{YouTube|user=UKinJapan|UKinJapan|British Embassy in Japan}} {{en icon}}',\n '*[https://www.gov.uk/government/organisations/uk-visas-and-immigration UK Visas and Immigration (GOV.UK)] {{en icon}}',\n '**[https://www.vfsglobal.co.uk/ja-jp 英国ビザ申請センター] - VFS Global Japan (上記「UK Visas and Immigration」日本地区取扱代行サイト) {{ja icon}}{{en icon}}',\n ';日本政府内',\n '*[https://www.mofa.go.jp/mofaj/area/uk/ 日本外務省 - 英国] {{ja icon}}',\n '*[https://www.uk.emb-japan.go.jp/ 在英国日本国大使館] {{ja icon}}',\n ';観光',\n '*[https://www.visitbritain.com/jp/ja 英国政府観光庁（日本語版サイト）] {{ja icon}}',\n '**{{Facebook|LoveGreatBritain|Love GREAT Britain}} {{en icon}}',\n ';その他',\n '*[https://www.jetro.go.jp/world/europe/uk/ JETRO - 英国] {{ja icon}}',\n '*{{CIA World Factbook link|uk|United Kingdom}} {{en icon}}',\n '*{{dmoz|Regional/Europe/United_Kingdom}} {{en icon}}',\n '*{{wikiatlas|United Kingdom}} {{en icon}}',\n '*{{Osmrelation|62149}}',\n '',\n '{{Normdaten}}',\n '{{イギリス関連の項目}}',\n '{{ヨーロッパ}}',\n '{{国連安全保障理事会理事国}}',\n '{{G8}}',\n '{{OECD}}',\n '{{イギリス連邦}}',\n '{{EU|1973年 - 2020年}}',\n '{{CPLP}}',\n '{{デフォルトソート:いきりす}}',\n '[[Category:イギリス|*]]',\n '[[Category:イギリス連邦加盟国]]',\n '[[Category:英連邦王国|*]]',\n '[[Category:G8加盟国]]',\n '[[Category:欧州連合加盟国|元]]',\n '[[Category:海洋国家]]',\n '[[Category:現存する君主国]]',\n '[[Category:島国]]',\n '[[Category:1801年に成立した国家・領域]]']"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "df_uk = df[df.title==\"イギリス\"]\n",
    "uk_texts = df_uk.text.values[0]\n",
    "uk_texts.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. カテゴリ名を含む行を抽出 \n",
    "記事中でカテゴリ名を宣言している行を抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [txt for txt in uk_texts.split(\"\\n\") if \"[Category:\" in txt]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u++ -> https://upura.hatenablog.com/entry/2020/04/14/232520\n",
    "# filter(condition(like lambda), list) -> pick out only True data in list\n",
    "list(filter(\n",
    "    lambda x: \"Category:\" in x, uk_texts.split(\"\\n\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. カテゴリ名の抽出 \n",
    "記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ebd2c441e906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\\[\\],.'\\\"\\|*=a-zA-Z{}]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "[re.sub(\"[\\[\\],.'\\\"\\|*=a-zA-Z{}]\", \"\", cat).split(\":\")[1] for cat in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ボクは欲望と', '可能性を食べました。']"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "<html> \n",
    "<head> \n",
    "<meta charset=\"utf-8\" />\n",
    "<title>前略プロフィール</title>\n",
    "</head> \n",
    "<body>\n",
    " <h2>昨日の夜ご飯</h2>\n",
    "<ol> <li>ボクは　欲望と</li> \n",
    "<li>可能性を　食べました。</li> \n",
    "</ol> </body> </html>\n",
    "\"\"\"\n",
    "\n",
    "re.findall(r\"(?<=<li>)(.+?)(?=</li>)\", \n",
    "           re.sub(r\"[\\u3000 \\t]\", \"\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u++ ref -> https://upura.hatenablog.com/entry/2020/04/14/232826\n",
    "df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "ukText = df.query(\"title=='イギリス'\")[\"text\"].values[0]\n",
    "ukTextList = ukText.split(\"\\n\")\n",
    "cats = list(filter(lambda x: \"Category:\" in x, ukTextList))\n",
    "cats = [cat.replace(\"[[Category:\", \"\").replace(\"|*\", \"\").replace(\"]]\", \"\") for cat in cats]\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. セクション構造 \n",
    "記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {\"=\"*(n+1)+\"\":n for n in range(5,0,-1)}\n",
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level_sections = {}\n",
    "added = []\n",
    "for key, level in levels.items():\n",
    "    sections = [txt for txt in uk_texts.split(\"\\n\") if key in txt]\n",
    "    try:\n",
    "        level_sections[level] = [ section for section in set(sections) if section not in added ]\n",
    "        added += level_sections[level]\n",
    "    except:\n",
    "        level_sections[level] = section\n",
    "print(pd.DataFrame.from_dict(level_sections, orient=\"index\").fillna(\"\").T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# これよくわからなかった（何がどうなれば正解なのか）\n",
    "# u++ ref -> https://upura.hatenablog.com/entry/2020/04/14/233246\n",
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "ukText = df.query(\"title=='イギリス'\")[\"text\"].values[0]\n",
    "for section in re.findall(r\"(=+)([^=]+)\\1\\n\", ukText):\n",
    "    print(f\"{section[1].strip()}\\t{len(section[0]) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# re.findalll(pattern, string, flags=0) -> https://itsakura.com/python-findall\n",
    "reg_exps = [r\"(=+)([^=]+)\\1\\n\", \n",
    "#             r\"(=+)([^=]+)\\n\",\n",
    "            r\"(=+)\",\n",
    "            r\"(=+)\\1\",\n",
    "            r\"(=+)\\n\",\n",
    "            r\"(=+)\\1\\n\",\n",
    "#             r\"([^=]+)\",\n",
    "#             r\"([^=]+)\\1\", \n",
    "#             r\"([^=]+)\\n\",\n",
    "            r\"([^=]+)\\1\\n\"\n",
    "           ]\n",
    "for reg_exp in reg_exps:\n",
    "    print(\"● \", reg_exp, \" ●\\n\", re.findall(reg_exp, ukText), \"\\n\\n\")\n",
    "    \n",
    "\"\"\"\n",
    "ここで使用されている正規表現について -> https://murashun.jp/blog/20190215-01.html\n",
    "(...) -> 文字を1つのグループにまとめる\n",
    "=+ -> +は直前の文字が1回以上繰り返す場合にマッチする（=+ だと、=が一回以上マッチする）\n",
    "^= -> ^は直後の文字が行の先頭にある場合にマッチする（^= だと、=が行の先頭に存在する時にマッチ）\n",
    "[^...] -> 角かっこ内に含まれる文字 以外 にマッチする（[^=]だと、 = を含まない文字列にマッチする）\n",
    "\\1 -> マッチした文字列の1文字目に対応する文字列に置換する（ここでは、一つ目の(=+)と同じ文字列が出現した場合）\n",
    "\\n -> 改行（textの中にベタ書きされている\\nという文字列、特別な意味があるわけではない）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\1 とか \\2 とかの振る舞い確認用\n",
    "def test():\n",
    "    text = \"qwert====##asdfg##\\nlkjh##====\\npoiuy\\nzxcvbnm\"\n",
    "    regular_expressions = [\n",
    "        r\"(=+)([^=]+)\\1\\n\",\n",
    "        r\"(=+)(#+)([^=]+)\\1\\n\",\n",
    "        r\"(=+)(#+)([^=]+)\\2\\1\\n\",\n",
    "        r\"(=+)(#+)([^=#]+)\\2\\1\\n\",\n",
    "        r\"(=+)(#+)([^=#]+)\\2\\n\",\n",
    "        r\"(#+)([^#]+)\\1\\n\",\n",
    "    ]\n",
    "    print(\"raw text: \", repr(text), \"\\n\")\n",
    "    for regular_expression in regular_expressions:\n",
    "        print(regular_expression, \"\\t\", re.findall(regular_expression, text))\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. ファイル参照の抽出 \n",
    "記事から参照されているメディアファイルをすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[txt for txt in uk_texts.split(\"\\n\") if \"<ref>\" in txt]\n",
    "# メディアファイル、か、、、ハイパーリンクだけのことじゃなかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u++ ref -> https://upura.hatenablog.com/entry/2020/04/14/233601\n",
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "ukText = df.query(\"title=='イギリス'\")[\"text\"].values[0]\n",
    "for file in re.findall(r\"\\[\\[(ファイル|File):([^]|]+?)(\\|.*?)+\\]\\]\", ukText):\n",
    "    print(file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exps = [r\"\\[\\[(ファイル|File):([^]|]+?)(\\|.*?)+\\]\\]\",\n",
    "            r\"\\[\\[(ファイル|File):([^]|]+?)\\]\\]\",\n",
    "            r\"\\[\\[(ファイル|File):\\]\\]\",\n",
    "           ]\n",
    "for reg_exp in reg_exps:\n",
    "    print(\n",
    "        \"● \", reg_exp, \" ●\\n\", re.findall(reg_exp, ukText), \"\\n\\n\",\n",
    "    )\n",
    "\"\"\"\n",
    "ここで使用されている正規表現について -> https://murashun.jp/blog/20190215-01.html\n",
    "(...) -> 文字を1つのグループにまとめる\n",
    ". -> 任意の一文字\n",
    "* -> 直前の文字が０回以上（最長一致）\n",
    "+ -> 直前の文字が１回以上（最長一致）\n",
    "=+ -> +は直前の文字が1回以上繰り返す場合にマッチする（=+ だと、=が一回以上マッチする）\n",
    "+? -> 直前の文字が１回以上繰り返す場合、この場合は ] が１回以上（最短一致）\n",
    "*? -> 直前の文字が０回以上繰り返す場合、この場合は .(任意の一文字) が０回以上（最短一致）\n",
    "| -> or演算子、含まれる文字が ファイル か File だったらマッチ、[^] か ] かだったらマッチ、って感じ\n",
    "^= -> ^は直後の文字が行の先頭にある場合にマッチする（^= だと、=が行の先頭に存在する時にマッチ）\n",
    "[...] -> 角かっこ内に含まれる何かしらの1文字にマッチ\n",
    "[^...] -> 角かっこ内に含まれる文字 以外 にマッチする（[^=]だと、 = を含まない文字列にマッチする）\n",
    "\n",
    "つまり、抽出しているのは、\n",
    "[[ (ファイル or File):([ ( ]が行の先頭 or ]が１個以上)) (|の後に何かしらの文字が０文字以上)が１文字以上 ]]\n",
    "となっている部分で、それぞれのかっこ（）内がfile[0:2]に順番に格納される\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. テンプレートの抽出 \n",
    "記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抽出したいのはこれ\n",
    "```python\n",
    "{{基礎情報 国\n",
    "|略名  =イギリス\n",
    "|日本語国名 = グレートブリテン及び北アイルランド連合王国\n",
    "|公式国名 = {{lang|en|United Kingdom of Great Britain and Northern Ireland}}<ref>英語以外での正式国名:<br />\n",
    "*{{lang|gd|An Rìoghachd Aonaichte na Breatainn Mhòr agus Eirinn mu Thuath}}（[[スコットランド・ゲール語]]）\n",
    "*{{lang|cy|Teyrnas Gyfunol Prydain Fawr a Gogledd Iwerddon}}（[[ウェールズ語]]）\n",
    "*{{lang|ga|Ríocht Aontaithe na Breataine Móire agus Tuaisceart na hÉireann}}（[[アイルランド語]]）\n",
    "*{{lang|kw|An Rywvaneth Unys a Vreten Veur hag Iwerdhon Glédh}}（[[コーンウォール語]]）\n",
    "*{{lang|sco|Unitit Kinrick o Great Breetain an Northren Ireland}}（[[スコットランド語]]）\n",
    "**{{lang|sco|Claught Kängrick o Docht Brätain an Norlin Airlann}}、{{lang|sco|Unitet Kängdom o Great Brittain an Norlin Airlann}}（アルスター・スコットランド語）</ref>\n",
    "|国旗画像 = Flag of the United Kingdom.svg\n",
    "|国章画像 = [[ファイル:Royal Coat of Arms of the United Kingdom.svg|85px|イギリスの国章]]\n",
    "|国章リンク =（[[イギリスの国章|国章]]）\n",
    "|標語 = {{lang|fr|[[Dieu et mon droit]]}}<br />（[[フランス語]]:[[Dieu et mon droit|神と我が権利]]）\n",
    "|国歌 = [[女王陛下万歳|{{lang|en|God Save the Queen}}]]{{en icon}}<br />''神よ女王を護り賜え''<br />{{center|[[ファイル:United States Navy Band - God Save the Queen.ogg]]}}\n",
    "|地図画像 = Europe-UK.svg\n",
    "|位置画像 = United Kingdom (+overseas territories) in the World (+Antarctica claims).svg\n",
    "|公用語 = [[英語]]\n",
    "|首都 = [[ロンドン]]（事実上）\n",
    "|最大都市 = ロンドン\n",
    "|元首等肩書 = [[イギリスの君主|女王]]\n",
    "|元首等氏名 = [[エリザベス2世]]\n",
    "|首相等肩書 = [[イギリスの首相|首相]]\n",
    "|首相等氏名 = [[ボリス・ジョンソン]]\n",
    "|他元首等肩書1 = [[貴族院 (イギリス)|貴族院議長]]\n",
    "|他元首等氏名1 = [[:en:Norman Fowler, Baron Fowler|ノーマン・ファウラー]]\n",
    "|他元首等肩書2 = [[庶民院 (イギリス)|庶民院議長]]\n",
    "|他元首等氏名2 = {{仮リンク|リンゼイ・ホイル|en|Lindsay Hoyle}}\n",
    "|他元首等肩書3 = [[連合王国最高裁判所|最高裁判所長官]]\n",
    "|他元首等氏名3 = [[:en:Brenda Hale, Baroness Hale of Richmond|ブレンダ・ヘイル]]\n",
    "|面積順位 = 76\n",
    "|面積大きさ = 1 E11\n",
    "|面積値 = 244,820\n",
    "|水面積率 = 1.3%\n",
    "|人口統計年 = 2018\n",
    "|人口順位 = 22\n",
    "|人口大きさ = 1 E7\n",
    "|人口値 = 6643万5600<ref>{{Cite web|url=https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates|title=Population estimates - Office for National Statistics|accessdate=2019-06-26|date=2019-06-26}}</ref>\n",
    "|人口密度値 = 271\n",
    "|GDP統計年元 = 2012\n",
    "|GDP値元 = 1兆5478億<ref name=\"imf-statistics-gdp\">[http://www.imf.org/external/pubs/ft/weo/2012/02/weodata/weorept.aspx?pr.x=70&amp;pr.y=13&amp;sy=2010&amp;ey=2012&amp;scsm=1&amp;ssd=1&amp;sort=country&amp;ds=.&amp;br=1&amp;c=112&amp;s=NGDP%2CNGDPD%2CPPPGDP%2CPPPPC&amp;grp=0&amp;a=IMF&gt;Data and Statistics>World Economic Outlook Databases>By Countrise>United Kingdom]</ref>\n",
    "|GDP統計年MER = 2012\n",
    "|GDP順位MER = 6\n",
    "|GDP値MER = 2兆4337億<ref name=\"imf-statistics-gdp\" />\n",
    "|GDP統計年 = 2012\n",
    "|GDP順位 = 6\n",
    "|GDP値 = 2兆3162億<ref name=\"imf-statistics-gdp\" />\n",
    "|GDP/人 = 36,727<ref name=\"imf-statistics-gdp\" />\n",
    "|建国形態 = 建国\n",
    "|確立形態1 = [[イングランド王国]]／[[スコットランド王国]]<br />（両国とも[[合同法 (1707年)|1707年合同法]]まで）\n",
    "|確立年月日1 = 927年／843年\n",
    "|確立形態2 = [[グレートブリテン王国]]成立<br />（1707年合同法）\n",
    "|確立年月日2 = 1707年{{0}}5月{{0}}1日\n",
    "|確立形態3 = [[グレートブリテン及びアイルランド連合王国]]成立<br />（[[合同法 (1800年)|1800年合同法]]）\n",
    "|確立年月日3 = 1801年{{0}}1月{{0}}1日\n",
    "|確立形態4 = 現在の国号「'''グレートブリテン及び北アイルランド連合王国'''」に変更\n",
    "|確立年月日4 = 1927年{{0}}4月12日\n",
    "|通貨 = [[スターリング・ポンド|UKポンド]] (£)\n",
    "|通貨コード = GBP\n",
    "|時間帯 = ±0\n",
    "|夏時間 = +1\n",
    "|ISO 3166-1 = GB / GBR\n",
    "|ccTLD = [[.uk]] / [[.gb]]<ref>使用は.ukに比べ圧倒的少数。</ref>\n",
    "|国際電話番号 = 44\n",
    "|注記 = <references/>\n",
    "}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u++ ref -> https://upura.hatenablog.com/entry/2020/04/14/234205\n",
    "import re\n",
    "import pandas as pd\n",
    "def knock_25():\n",
    "    df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "    ukText = df.query(\"title=='イギリス'\")[\"text\"].values[0]\n",
    "    ls, fg = [], False\n",
    "    template = \"基礎情報\"\n",
    "    \n",
    "    p1 = re.compile(\"\\{\\{\" + template)\n",
    "    p2 = re.compile(\"\\}\\}\")\n",
    "    p3 = re.compile(\"\\|\")\n",
    "    p4 = re.compile(\"<ref(\\s|>).+?(</ref>|$)\") # $ ... 直前の文字が末尾にある場合\n",
    "    for l in ukText.split(\"\\n\"):\n",
    "        if fg:\n",
    "            ml = [p2.match(l), p3.match(l)]\n",
    "            if ml[0]:\n",
    "                break\n",
    "            if ml[1]:\n",
    "                ls.append(p4.sub('', l.strip()))\n",
    "        if p1.match(l):\n",
    "            fg = True\n",
    "    p = re.compile(\"\\|(.+?)\\s=\\s(.+)\")\n",
    "    template_dict = {m.group(1):m.group(2) for m in [p.match(c) for c in ls]}\n",
    "    print(template_dict)\n",
    "\n",
    "knock_25() # raise Attribute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ref Qiita -> https://qiita.com/yamaru/items/255d0c5dcb2d1d4ccc14#25-テンプレートの抽出\n",
    "def knock_25():\n",
    "    df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "    text_uk = df.query(\"title=='イギリス'\")[\"text\"].values[0]\n",
    "    \n",
    "    # テンプレートの抽出\n",
    "    pattern = r'^\\{\\{基礎情報.*?$(.*?)^\\}\\}'\n",
    "    template = re.findall(pattern, text_uk, re.MULTILINE + re.DOTALL)\n",
    "    print(template)\n",
    "\n",
    "    # フィールド名と値を辞書オブジェクトに格納\n",
    "    pattern = r'^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))'\n",
    "    result = dict(re.findall(pattern, template[0], re.MULTILINE + re.DOTALL))\n",
    "    for k, v in result.items():\n",
    "        print(k + ': ' + v + \"\\n\")\n",
    "knock_25()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. 強調マークアップの除去 \n",
    "25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ（参考: [マークアップ早見表](http://ja.wikipedia.org/wiki/Help:早見表)）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u++ ref -> https://upura.hatenablog.com/entry/2020/04/14/235025\n",
    "import re\n",
    "import pandas as pd\n",
    "def knock_26():\n",
    "    # make same dict with knock25\n",
    "    df = pd.read_json(\"section03/jawiki-country.json\", lines=True)\n",
    "    text_uk = df.query(\"title=='イギリス'\")[\"text\"].values[0]\n",
    "    pattern = r'^\\{\\{基礎情報.*?$(.*?)^\\}\\}'\n",
    "    template = re.findall(pattern, text_uk, re.MULTILINE + re.DOTALL)\n",
    "    pattern = r'^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))'\n",
    "    result = dict(re.findall(pattern, template[0], re.MULTILINE + re.DOTALL))\n",
    "    \n",
    "    remove_stress = re.compile(r\"'+\")\n",
    "    return {k: remove_stress.sub(\"\", v) for k, v in result.items()}\n",
    "\n",
    "knock_26()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. 内部リンクの除去 \n",
    "26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: [マークアップ早見表](http://ja.wikipedia.org/wiki/Help:早見表)）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[[ ]]`の括弧のペアを除去することを考える（[wikiの内部リンクについて](https://www.mediawiki.org/wiki/Help:Links/ja)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "def test():\n",
    "    text = {'通貨': '[[スターリング・ポンド|UKポンド]] (£)',\n",
    "            '通貨コード': 'GBP',\n",
    "            '時間帯': '±0',\n",
    "            '夏時間': '+1',\n",
    "            'ISO 3166-1': 'GB / GBR',\n",
    "            'ccTLD': '[[.uk]] / [[.gb]]<ref>使用は.ukに比べ圧倒的少数。</ref>',\n",
    "            '国際電話番号': '44',\n",
    "            '注記': '<references/>'\n",
    "           }\n",
    "    \n",
    "    #rm_mu = re.compile(\"\\[\\[(.*)*\\](\\]$)\")\n",
    "    rm_heads = re.compile(\"\\[\\[\")\n",
    "    rm_tails = re.compile(\"\\]\\]\")\n",
    "    \n",
    "    for k, v in text.items():\n",
    "        print(\"k=\",k,\", v=\",v)\n",
    "        #print(rm_mu.sub(\"\",v))\n",
    "        if rm_heads.match(v):\n",
    "            print(\"  \", rm_heads.sub(\"\",v))\n",
    "            print(\"  \", rm_tails.sub(\"\", rm_heads.sub(\"\",v)))\n",
    "        else:\n",
    "            print(\"  No regular expressions are matched.\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u++ ref -> https://upura.hatenablog.com/entry/2020/04/14/235509\n",
    "import re\n",
    "import pandas as pd\n",
    "def knock_27():\n",
    "    NoStress_26 = knock_26()\n",
    "    remove_inner_markup = re.compile(\"\\[\\[.*\\]\\]\")\n",
    "    \n",
    "    re.findall(r\"\", NoStress_26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. MediaWikiマークアップの除去 \n",
    "27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29. 国旗画像のURLを取得する \n",
    "テンプレートの内容を利用し，国旗画像のURLを取得せよ．（ヒント: [MediaWiki API](http://www.mediawiki.org/wiki/API:Main_page/ja)の[imageinfo](https://www.mediawiki.org/wiki/API:Imageinfo)を呼び出して，ファイル参照をURLに変換すればよい）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第４章 形態素解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
    "\n",
    "なお，問題37, 38, 39は[matplotlib](http://matplotlib.org/)もしくは[Gnuplot](Gnuplot)を用いるとよい\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://nlp100.github.io/data/neko.txt -P ./section04/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory of dic file; mecab-ipadic-neologd\n",
    "```\n",
    "root@8583172500da:/usr/lib/x86_64-linux-gnu/mecab/dic# ls\n",
    "mecab-ipadic-neologd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neko.txt.mecab ref: https://taku910.github.io/mecab/#usage-tools\n",
    "!mecab section04/neko.txt -o section04/neko.txt.mecab -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. 形態素解析結果の読み込み \n",
    "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．  \n",
    "\n",
    "----\n",
    "mecab returns as below;  \n",
    "表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import MeCab\n",
    "\n",
    "def load_mecab_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        mecab_file = f.readlines()\n",
    "    \n",
    "    mecab_file = mecab_file[1:] # initial line is header\n",
    "    mecab_file = [s for s in mecab_file if s not in [\"\\n\", \"EOS\\n\"]] # remove lines if only \\n or EOS\\n\n",
    "    mecab_file = [s for s in mecab_file if len(s.split(\"\\t\")[0])>0] # remove lines if target word is \\n\n",
    "    mecab_file = [s for s in mecab_file if s.split(\"\\t\")[0]!=\"\\u3000\"] # remove lines if target word is \\u3000\n",
    "\n",
    "    mecab_file = [s.replace(\"\\n\", \"\") for s in mecab_file] # replace \"\\n\" into \"\"\n",
    "    mecab_file = [s.replace(\"\\t\", \",\") for s in mecab_file] # replace \"\\t\" into \",\"\n",
    "    \n",
    "    return mecab_file\n",
    "\n",
    "def morpheme_dict(sentence,i=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        An input sentence formed as below;\n",
    "            表層形,品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
    "    i : int, default None\n",
    "        For debug, if i==None then raise no assertion error.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    　ここでは、形態素解析した文章をカンマ区切りで分割し、それをマッピングしたdictを返している。\n",
    "    　特別な名詞（頸筋、など）の場合、読み仮名が登録されておらず、形態素解析の結果がそもそも10個ではなく\n",
    "    8個しか要素を持っていない、というケースがある。その場合、未知（UNKnown）を意味する'UNK'を代わりに代入している。\n",
    "    (このデバッグ作業を実施するときにのみ、argsのiにfor loopのinteration No.を渡す)\n",
    "    \"\"\"\n",
    "    s = sentence.split(\",\")\n",
    "    \n",
    "    if len(s)!=10:\n",
    "        for _ in range(10 - len(s)):\n",
    "            s.append(\"UNK\")\n",
    "    \n",
    "    # '頸筋,名詞,一般,*,*,*,*,*',\n",
    "    # 特別な名詞（読み仮名が設定されていないもの）はそもそも出力値が10個にならない！\n",
    "    if i is not None: assert len(s)==10, f\"sentence hasn't enough components; i={i}, sentence={s}\"\n",
    "\n",
    "    return OrderedDict({\"surface\": s[0],\n",
    "            \"pos\": s[1],\n",
    "            \"pos1\": s[2],\n",
    "            \"pos2\": s[3],\n",
    "            \"pos3\": s[4],\n",
    "            \"conj\": s[5],\n",
    "            \"type\": s[6],\n",
    "            \"orig\": s[7],\n",
    "            \"read\": s[8],\n",
    "            \"pron\": s[9]\n",
    "            })\n",
    "\n",
    "def knock_030():\n",
    "    fmecab = load_mecab_file(\"section04/neko.txt.mecab\")\n",
    "    # pprint(fmecab[845:855])\n",
    "    fmecab = [morpheme_dict(s) for i, s in enumerate(fmecab)]\n",
    "    return fmecab\n",
    "\n",
    "fmec = knock_030()\n",
    "pprint(fmec[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(fmec)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. 動詞 \n",
    "動詞の表層形をすべて抽出せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pos==\"動詞\"][\"surface\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_word(morpheme_list, extract_key, **conditions):\n",
    "    _key = [_ for _ in conditions.keys()]\n",
    "    assert len(_key)==1, f\"The allowed number of input key for extraction is 1 (not {len(_key)}).\"\n",
    "    _key = _key[0]\n",
    "    _val = [_ for _ in conditions.values()][0]\n",
    "    return [m[extract_key] for m in morpheme_list if m[_key]==_val]\n",
    "\n",
    "def knock_031():\n",
    "    fmec = knock_030()\n",
    "    return extract_word(fmec, extract_key=\"surface\", pos=\"動詞\")\n",
    "\n",
    "knock_031()[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. 動詞の原形 \n",
    "動詞の原形をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pos==\"動詞\"][\"orig\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_032():\n",
    "    fmec = knock_030()\n",
    "    return extract_word(fmec, extract_key=\"orig\", pos=\"動詞\")\n",
    "\n",
    "knock_032()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. 「AのB」 \n",
    "2つの名詞が「の」で連結されている名詞句を抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pos.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pos==\"名詞\"][\"pos1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_033():\n",
    "    fmec = knock_030()\n",
    "    extracts = []\n",
    "    for i in range(len(fmec)-2):\n",
    "        if fmec[i+1][\"surface\"]==\"の\":\n",
    "            if fmec[i][\"pos\"]==\"名詞\" and fmec[i+2][\"pos\"]==\"名詞\":\n",
    "                extracts.append(fmec[i][\"surface\"]+fmec[i+1][\"surface\"]+fmec[i+2][\"surface\"])\n",
    "    return extracts\n",
    "\n",
    "knock_033()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. 名詞の連接\n",
    "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_34():\n",
    "    fmec = knock_030()\n",
    "    extracts = []\n",
    "    i = 0\n",
    "    while i < len(fmec):\n",
    "        if fmec[i][\"pos\"]==\"名詞\":\n",
    "            j = 1\n",
    "            while True:\n",
    "                if fmec[i+j][\"pos\"]==\"名詞\":\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "            if j >= 2:\n",
    "                noun = \"\"\n",
    "                for k in range(i,i+j):\n",
    "                    noun += fmec[k][\"surface\"]\n",
    "                extracts.append(noun)\n",
    "        i += j\n",
    "    return exctracts\n",
    "\n",
    "knock_34()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. 単語の出現頻度\n",
    "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"surface\"].value_counts().items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36. 頻度上位10語\n",
    "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"surface\"].value_counts().head(10).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37. 「猫」と共起頻度の高い上位10語\n",
    "「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uppさんの回答を参照するに、狂気というのは近くで使用される単語、というわけではなくて、同じ文章中で使用されている単語、という意味らしい。違うことやってたわワロタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def upp_answer():\n",
    "    def parse_mecab(block):\n",
    "        res = []\n",
    "        for line in block.split('\\n'):\n",
    "            if line == '':\n",
    "                return res\n",
    "            (surface, attr) = line.split('\\t')\n",
    "            attr = attr.split(',')\n",
    "            lineDict = {\n",
    "                'surface': surface,\n",
    "                'base': attr[6],\n",
    "                'pos': attr[0],\n",
    "                'pos1': attr[1]\n",
    "            }\n",
    "            res.append(lineDict)\n",
    "\n",
    "\n",
    "    def extract_base(block):\n",
    "        return [b['base'] for b in block]\n",
    "\n",
    "\n",
    "    filename = 'section04/neko.txt.mecab'\n",
    "    with open(filename, mode='rt', encoding='utf-8') as f:\n",
    "        blocks = f.read().split('EOS\\n')\n",
    "    blocks = list(filter(lambda x: x != '', blocks)); print(blocks[1])\n",
    "    blocks = [parse_mecab(block) for block in blocks]; print(blocks[0])\n",
    "    words = [extract_base(block) for block in blocks]; print(words[0])\n",
    "    words = list(filter(lambda x: '猫' in x, words)); print(words[0])\n",
    "    d = defaultdict(int)\n",
    "    for word in words:\n",
    "        for w in word:\n",
    "            if w != '猫':\n",
    "                d[w] += 1\n",
    "    ans = sorted(d.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    labels = [a[0] for a in ans]\n",
    "    values = [a[1] for a in ans]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.barh(labels, values)\n",
    "upp_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def knock_037(n_range=2):\n",
    "    assert n_range>0, 'n_range shoud be set >=1.'\n",
    "    fmec = knock_030()\n",
    "    extracts = {}\n",
    "    df = pd.DataFrame.from_dict(fmec)\n",
    "    fmec\n",
    "    for i, suf in enumerate(df[\"surface\"].values):\n",
    "        if suf==\"猫\":\n",
    "            # 前後何個の形態素を参照するかを決定している部分\n",
    "            # 形態素群の先頭/末尾の n_range 個以内であれば、フルに指定した版を参照できるわけではにため、\n",
    "            # ループの i と比較してその範囲を限定している\n",
    "            n_tail = [-1*k for k in range(1, min(i, n_range)+1)] if i!=0 else []\n",
    "            n_head = [k for k in range(1, min(len(fmec)-i, n_range)+1)] if i!=len(fmec) else []\n",
    "            n_list = n_tail + n_head\n",
    "\n",
    "            for n in n_list:\n",
    "                # 上で決定した範囲の形態素を参照していく、もし既に猫の近傍に存在していた単語なら、その出現回数をインクリメントする\n",
    "                # もしも初登場であったのなら、新たに作成する\n",
    "                try:\n",
    "                    extracts[fmec[i+n][\"surface\"]] += 1\n",
    "                except KeyError:\n",
    "                    extracts[fmec[i+n][\"surface\"]] = 1\n",
    "    return pd.DataFrame(extracts, index=[\"freq\"]).T\n",
    "\n",
    "df = knock_037(n_range=1)\n",
    "df.sort_values(by=\"freq\", ascending=False).head(10).plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前後数単語、じゃなくて一つの文章中で一緒に使用されている単語、に変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def morpheme_dict_037(sentence,i=None):\n",
    "    s_block = []\n",
    "    for s in sentence.split('\\n'):\n",
    "        if s=='': return s_block\n",
    "        s = s.replace('\\t', ',').split(\",\")\n",
    "        \n",
    "        if len(s)!=10:\n",
    "            for _ in range(10 - len(s)):\n",
    "                s.append(\"UNK\")\n",
    "        \n",
    "        # '頸筋,名詞,一般,*,*,*,*,*',\n",
    "        # 特別な名詞（読み仮名が設定されていないもの）はそもそも出力値が10個にならない！\n",
    "        if i is not None: assert len(s)==10, f\"sentence hasn't enough components; i={i}, sentence={s}\"\n",
    "\n",
    "        s_block.append(\n",
    "            OrderedDict({\"surface\": s[0],\n",
    "                \"pos\": s[1],\n",
    "                \"pos1\": s[2],\n",
    "                \"pos2\": s[3],\n",
    "                \"pos3\": s[4],\n",
    "                \"conj\": s[5],\n",
    "                \"type\": s[6],\n",
    "                \"orig\": s[7],\n",
    "                \"read\": s[8],\n",
    "                \"pron\": s[9]\n",
    "                })\n",
    "        )\n",
    "    else:\n",
    "        # print(s_block)\n",
    "        return s_block\n",
    "            \n",
    "def reconstruct_sentence(sentences):\n",
    "    return [morpheme_dict_037(sentence) for sentence in sentences]\n",
    "\n",
    "def extract_surface(morph, pos=None):\n",
    "    \"\"\"\n",
    "    pos に何かを入れることで、それと一致する瀕死の surface のみを抽出する\n",
    "    \"\"\"\n",
    "    if pos is None:\n",
    "        return [m['surface'] for m in morph]\n",
    "    else:\n",
    "        return [m['surface'] for m in morph if m['pos']==pos]\n",
    "\n",
    "\n",
    "def knock_037(corr_word='猫', pos=None):\n",
    "    # fmecab = load_mecab_file(\"section04/neko.txt.mecab\")\n",
    "    with open(\"section04/neko.txt.mecab\", \"r\", encoding='utf-8') as f:\n",
    "        mecab_sentence = f.read().split('EOS\\n')\n",
    "        # mecab_file = f.readlines() # 今回は一行毎にリストとして格納するのではなく、まとめて１文として読み込んで、それをEOSで分割する\n",
    "    # 連続した1文毎に `\\n,\\t,EOS\\n` の3つで区切られているため、これらを利用して一文毎にまとめる\n",
    "    mecab_sentence = list(filter(lambda x: x != '', mecab_sentence))\n",
    "    mecab_sentence = [s for s in mecab_sentence if s.split('\\n')[0] != '']\n",
    "\n",
    "    mecab_sentence = reconstruct_sentence(mecab_sentence)\n",
    "    surs = [extract_surface(ms, pos=pos) for ms in mecab_sentence]\n",
    "\n",
    "    surs = list(filter(lambda x: corr_word in x, surs))\n",
    "    d = defaultdict(int)\n",
    "    for sur in surs:\n",
    "        for s in sur:\n",
    "            if s != corr_word:\n",
    "                d[s] += 1\n",
    "    ans = sorted(d.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    labels = [a[0] for a in ans]\n",
    "    values = [a[1] for a in ans]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.barh(labels, values)\n",
    "\n",
    "knock_037(corr_word='猫', pos=None)\n",
    "knock_037(corr_word='名', pos='名詞')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38. ヒストグラム  \n",
    "単語の出現頻度のヒストグラム（ただし，横軸は出現頻度を表し，1から単語の出現頻度の最大値までの線形目盛とする．縦軸はx軸で示される出現頻度となった単語の異なり数（種類数）である）を描け。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def knock_038():\n",
    "    fmec = knock_030()\n",
    "    df = pd.Series([f['surface'] for f in fmec]) # extract sufaces\n",
    "    df = df.value_counts().value_counts().sort_index() # 単語の出現頻度を数えて、その出現頻度をさらに数える\n",
    "\n",
    "    xlabels = df.index.values.astype(str)\n",
    "    kinds = df.values\n",
    "\n",
    "    plt.bar(xlabels, kinds)\n",
    "    plt.show()\n",
    "\n",
    "    plt.bar(xlabels[:10], kinds[:10])\n",
    "    plt.show()\n",
    "\n",
    "knock_038()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39. Zipfの法則\n",
    "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．  \n",
    "ref; zipf's law -> [wiki](https://ja.wikipedia.org/wiki/ジップの法則)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_039():\n",
    "    fmec = knock_030()\n",
    "\n",
    "    df = pd.Series([f['surface'] for f in fmec]) # extract sufaces\n",
    "    df = df.value_counts().value_counts().sort_index() # 単語の出現頻度を数えて、その出現頻度をさらに数える\n",
    "\n",
    "    xlabels = [np.log1p(idx) for idx in df.index.values]\n",
    "    kinds = df.apply(np.log1p).values\n",
    "\n",
    "    plt.plot(xlabels, kinds)\n",
    "    plt.show()\n",
    "\n",
    "knock_039()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第５章 係り受け解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--2020-08-15 00:28:30--  https://nlp100.github.io/data/neko.txt\nResolving nlp100.github.io (nlp100.github.io)...185.199.109.153, 185.199.108.153, 185.199.110.153, ...\nConnecting to nlp100.github.io (nlp100.github.io)|185.199.109.153|:443...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 975789 (953K) [text/plain]\nSaving to: ‘./section05/neko.txt’\n\nneko.txt            100%[===================>] 952.92K   940KB/s    in 1.0s    \n\n2020-08-15 00:28:32 (940 KB/s) - ‘./section05/neko.txt’ saved [975789/975789]\n\n"
    }
   ],
   "source": [
    "!mkdir section05\n",
    "!wget https://nlp100.github.io/data/neko.txt -P ./section05/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import CaboCha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "parser.parse(sentence) ->  <CaboCha.Tree; proxy of <Swig Object of type 'CaboCha::Tree *' at 0x7fd2355c3a50> >\ntree = parser.parse(sentence) ->  <CaboCha.Tree; proxy of <Swig Object of type 'CaboCha::Tree *' at 0x7fd2355c3b40> >\n\ntree.toString(CaboCha.FORMAT_TREE) -> \n     吾輩は-D      \n  猫である。-----D\n          まだ---D\n            名は-D\n            ない。\nEOS\n\ntree.toString(CaboCha.FORMAT_LATTICE) -> \n * 0 1D 0/1 1.487499\n吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\nは\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n* 1 4D 0/2 -1.971376\n猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\nで\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\nある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n。\t記号,句点,*,*,*,*,。,。,。\n* 2 4D 0/0 -1.971376\nまだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\n* 3 4D 0/1 -1.971376\n名\t名詞,一般,*,*,*,*,名,ナ,ナ\nは\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n* 4 -1D 0/0 0.000000\nない\t助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ\n。\t記号,句点,*,*,*,*,。,。,。\nEOS\n\ntree.toString(CaboCha.FORMAT_XML) -> \n <sentence>\n <chunk id=\"0\" link=\"1\" rel=\"D\" score=\"1.487499\" head=\"0\" func=\"1\">\n  <tok id=\"0\" feature=\"名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\">吾輩</tok>\n  <tok id=\"1\" feature=\"助詞,係助詞,*,*,*,*,は,ハ,ワ\">は</tok>\n </chunk>\n <chunk id=\"1\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"2\" func=\"4\">\n  <tok id=\"2\" feature=\"名詞,一般,*,*,*,*,猫,ネコ,ネコ\">猫</tok>\n  <tok id=\"3\" feature=\"助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\">で</tok>\n  <tok id=\"4\" feature=\"助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\">ある</tok>\n  <tok id=\"5\" feature=\"記号,句点,*,*,*,*,。,。,。\">。</tok>\n </chunk>\n <chunk id=\"2\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"6\" func=\"6\">\n  <tok id=\"6\" feature=\"副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\">まだ</tok>\n </chunk>\n <chunk id=\"3\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"7\" func=\"8\">\n  <tok id=\"7\" feature=\"名詞,一般,*,*,*,*,名,ナ,ナ\">名</tok>\n  <tok id=\"8\" feature=\"助詞,係助詞,*,*,*,*,は,ハ,ワ\">は</tok>\n </chunk>\n <chunk id=\"4\" link=\"-1\" rel=\"D\" score=\"0.000000\" head=\"9\" func=\"9\">\n  <tok id=\"9\" feature=\"助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ\">ない</tok>\n  <tok id=\"10\" feature=\"記号,句点,*,*,*,*,。,。,。\">。</tok>\n </chunk>\n</sentence>\n\n\nparser.parseToString(sentence) -> \n     吾輩は-D      \n  猫である。-----D\n          まだ---D\n            名は-D\n            ない。\nEOS\n\ntree.token(i).feature for i in range(tree.size()) -> \n\t 名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n\t 助詞,係助詞,*,*,*,*,は,ハ,ワ\n\t 名詞,一般,*,*,*,*,猫,ネコ,ネコ\n\t 助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n\t 助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n\t 記号,句点,*,*,*,*,。,。,。\n\t 副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\n\t 名詞,一般,*,*,*,*,名,ナ,ナ\n\t 助詞,係助詞,*,*,*,*,は,ハ,ワ\n\t 助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ\n\t 記号,句点,*,*,*,*,。,。,。\n"
    }
   ],
   "source": [
    "def test(sentence):\n",
    "\n",
    "    parser = CaboCha.Parser('--charset=UTF8')\n",
    "    print('parser.parse(sentence) -> ', parser.parse(sentence))    \n",
    "    tree = parser.parse(sentence)\n",
    "    print('tree = parser.parse(sentence) -> ', tree)\n",
    "    print()\n",
    "\n",
    "    print('tree.toString(CaboCha.FORMAT_TREE) -> \\n', tree.toString(CaboCha.FORMAT_TREE))\n",
    "    print('tree.toString(CaboCha.FORMAT_LATTICE) -> \\n', tree.toString(CaboCha.FORMAT_LATTICE))\n",
    "    print('tree.toString(CaboCha.FORMAT_XML) -> \\n', tree.toString(CaboCha.FORMAT_XML))\n",
    "\n",
    "    print()\n",
    "    print('parser.parseToString(sentence) -> \\n', parser.parseToString(sentence))\n",
    "\n",
    "    print('tree.token(i).feature for i in range(tree.size()) -> ')\n",
    "    for i in range(tree.size()):\n",
    "        print('\\t', tree.token(i).feature)\n",
    "\n",
    "test('吾輩は猫である。まだ名はない。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cabocha_file(file):\n",
    "    \"\"\"\n",
    "    e.g. sentence = '吾輩は猫である。'\n",
    "    CaboCha analysis:\n",
    "        吾輩は-D      \n",
    "          猫である。-----D\n",
    "                  まだ---D\n",
    "                    名は-D\n",
    "                    ない。\n",
    "        EOS\n",
    "         \n",
    "    XML style: (ref -> https://qiita.com/nezuq/items/f481f07fc0576b38e81d)\n",
    "        <sentence> \n",
    "            # 文 sentence 始まり\n",
    "        <chunk id=\"0\" link=\"1\" rel=\"D\" score=\"1.487499\" head=\"0\" func=\"1\"> \n",
    "            # 文節 chunk 始まり (id; 文節番号、link; 係り先の文節番号、score; 係り関係のスコア（大きいほど係りやすい）、\n",
    "            # 　　　　　　　　　　 head; 主辞の形態素番号、func; 機能語の形態素番号)\n",
    "        <tok id=\"0\" feature=\"名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\">吾輩</tok>\n",
    "            # 形態素 tok 始まり (id; 形態素番号、feature;　品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
    "            # 　　　　　　　　　　値; 表層形)\n",
    "        <tok id=\"1\" feature=\"助詞,係助詞,*,*,*,*,は,ハ,ワ\">は</tok>\n",
    "        </chunk>\n",
    "        <chunk id=\"1\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"2\" func=\"4\">\n",
    "        <tok id=\"2\" feature=\"名詞,一般,*,*,*,*,猫,ネコ,ネコ\">猫</tok>\n",
    "        <tok id=\"3\" feature=\"助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\">で</tok>\n",
    "        <tok id=\"4\" feature=\"助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\">ある</tok>\n",
    "        <tok id=\"5\" feature=\"記号,句点,*,*,*,*,。,。,。\">。</tok>\n",
    "        </chunk>\n",
    "        <chunk id=\"2\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"6\" func=\"6\">\n",
    "        <tok id=\"6\" feature=\"副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\">まだ</tok>\n",
    "        </chunk>\n",
    "        <chunk id=\"3\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"7\" func=\"8\">\n",
    "        <tok id=\"7\" feature=\"名詞,一般,*,*,*,*,名,ナ,ナ\">名</tok>\n",
    "        <tok id=\"8\" feature=\"助詞,係助詞,*,*,*,*,は,ハ,ワ\">は</tok>\n",
    "        </chunk>\n",
    "        <chunk id=\"4\" link=\"-1\" rel=\"D\" score=\"0.000000\" head=\"9\" func=\"9\">\n",
    "        <tok id=\"9\" feature=\"助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ\">ない</tok>\n",
    "        <tok id=\"10\" feature=\"記号,句点,*,*,*,*,。,。,。\">。</tok>\n",
    "        </chunk>\n",
    "        </sentence>\n",
    "    \"\"\"\n",
    "    import CaboCha\n",
    "    parser = CaboCha.Parser('--charset=UTF8')\n",
    "    \n",
    "    with open(file, 'rt', encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "    \n",
    "    # XML形式で構造解析結果を保存\n",
    "    trees = [\n",
    "        parser.parse(sentence).toString(CaboCha.FORMAT_XML) for sentence in sentences\n",
    "        ]\n",
    "\n",
    "    with open(file + '.cabocha', 'w') as f:\n",
    "        f.writelines(trees)\n",
    "\n",
    "create_cabocha_file('./section05/neko.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Sequence, Tuple, Union, Any, TYPE_CHECKING\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, sentence: str):\n",
    "        self.sent = [s.strip() for s in sentence.split('\\n')]\n",
    "        self.toks = {}\n",
    "        self.surface = []\n",
    "        self.base = []\n",
    "        self.pos = []\n",
    "        self.pos1 = []\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        toks = self.extract_all_tok()\n",
    "        self.surface = toks['surface']\n",
    "        self.base = toks['base']\n",
    "        self.pos = toks['pos']\n",
    "        self.pos1 = toks['pos1']\n",
    "        self.toks = toks\n",
    "\n",
    "\n",
    "    def split_tok_sentence(self, sentence:str) -> Tuple:\n",
    "        \"\"\"\n",
    "            <tok id=\"2\" feature=\"名詞,一般,*,*,*,*,猫,ネコ,ネコ\">猫</tok>\n",
    "            feature;　品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
    "        \"\"\"\n",
    "        id = int(sentence.split(' ')[1].split('=')[1].replace('\"', ''))\n",
    "        surface = sentence.split('>')[1].split('<')[0]\n",
    "        feats = sentence.split('>')[0].split('=')[-1].replace('\"', '').split(',')\n",
    "        \n",
    "        return (id, surface, feats)\n",
    "                \n",
    "\n",
    "    def extract_all_tok(self) -> Dict:\n",
    "        toks = {'tokid': [], 'surface': [], 'base': [], 'pos': [], 'pos1': []}\n",
    "        for s in self.sent:\n",
    "            if s[:4] == '<tok':\n",
    "                id, surface, feats = self.split_tok_sentence(s)\n",
    "                toks['tokid'].append(id)\n",
    "                toks['surface'].append(surface)\n",
    "                toks['base'].append(feats[6])\n",
    "                toks['pos'].append(feats[0])\n",
    "                toks['pos1'].append(feats[1])\n",
    "\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'tokid': [0, 1, 2, 3, 4, 5, 6], 'surface': ['\\u3000', '吾輩', 'は', '猫', 'で', 'ある', '。'], 'base': ['\\u3000', '吾輩', 'は', '猫', 'だ', 'ある', '。'], 'pos': ['記号', '名詞', '助詞', '名詞', '助動詞', '助動詞', '記号'], 'pos1': ['空白', '代名詞', '係助詞', '一般', '*', '*', '句点']}\n"
    }
   ],
   "source": [
    "def knock_040():\n",
    "    with open('./section05/neko.txt.cabocha', 'rt', encoding='utf-8') as f:\n",
    "        sentences = f.read().replace('<sentence>', '').split('</sentence>')\n",
    "\n",
    "    neko_morphs = [\n",
    "        Morph(sentence) for sentence in sentences\n",
    "        ]\n",
    "    \n",
    "    for mor in neko_morphs:\n",
    "        mor()\n",
    "\n",
    "    print(neko_morphs[2].toks)\n",
    "\n",
    "knock_040()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    \"\"\"\n",
    "    A part of input sentence is shown below;\n",
    "        <chunk id=\"1\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"2\" func=\"4\">\n",
    "        <tok id=\"2\" feature=\"名詞,一般,*,*,*,*,猫,ネコ,ネコ\">猫</tok>\n",
    "        <tok id=\"3\" feature=\"助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\">で</tok>\n",
    "        <tok id=\"4\" feature=\"助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\">ある</tok>\n",
    "        <tok id=\"5\" feature=\"記号,句点,*,*,*,*,。,。,。\">。</tok>\n",
    "        </chunk>\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence: str):\n",
    "        self.sent = [s.strip() for s in sentence.split('</chunk>')]\n",
    "        self.chunks = {}\n",
    "        self.morphs = []\n",
    "        self.dst = [] # link\n",
    "        self.srcs = [] # is this mean the chunk id ??\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        chunks = self.extract_all_chunk()\n",
    "        self.morphs = chunks['morph']\n",
    "        self.dst = chunks['dst']\n",
    "        self.srcs = chunks['srcs']\n",
    "        self.chunks = chunks\n",
    "\n",
    "\n",
    "    def split_chunk_sentence(self, sentence: str) -> Tuple:\n",
    "        \"\"\"\n",
    "            <chunk id=\"1\" link=\"4\" rel=\"D\" score=\"-1.971376\" head=\"2\" func=\"4\">\n",
    "                id; 文節番号、link; 係り先の文節番号、score; 係り関係のスコア（大きいほど係りやすい）、\n",
    "                head; 主辞の形態素番号、func; 機能語の形態素番号\n",
    "        \"\"\"\n",
    "        segments = sentence.split(' ')\n",
    "        id    = int(segments[1].split('=')[1].replace('\"', ''))\n",
    "        link  = int(segments[2].split('=')[1].replace('\"', ''))\n",
    "        rel   =     segments[3].split('=')[1].replace('\"', '')\n",
    "        score = float(segments[4].split('=')[1].replace('\"', ''))\n",
    "        head  = int(segments[5].split('=')[1].replace('\"', ''))\n",
    "        func  = int(segments[6].split('=')[1].replace('\"', '').rstrip('>'))\n",
    "        \n",
    "        return (id, link)\n",
    "                \n",
    "\n",
    "    def extract_all_chunk(self) -> Dict:\n",
    "        chunks = {'chunkid': [], 'morph': [], 'dst': [], 'srcs': []}\n",
    "        for s in self.sent: # splited by '</chunk>'\n",
    "            if len(s)==0: continue\n",
    "            (chunk_sent, tok_sent) = s.split('\\n', 1)\n",
    "            src, dst = self.split_chunk_sentence(chunk_sent)\n",
    "\n",
    "            mor = Morph(tok_sent)\n",
    "            mor()\n",
    "\n",
    "            chunks['chunkid'].append(src) # chunk_id == srcs ...???\n",
    "            chunks['srcs'].append(src)\n",
    "            chunks['dst'].append(dst)\n",
    "            chunks['morph'].append(mor)\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(chunks: Dict):\n",
    "    for i, id in enumerate(chunks['chunkid']):\n",
    "        print(f\"chunkid={chunks['chunkid'][i]}; src={chunks['srcs'][i]}, dst={chunks['dst'][i]}, \\n\\t morph={chunks['morph'][i].toks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "chunkid=0; src=0, dst=5, \n\t morph={'tokid': [0, 1], 'surface': ['吾輩', 'は'], 'base': ['吾輩', 'は'], 'pos': ['名詞', '助詞'], 'pos1': ['代名詞', '係助詞']}\nchunkid=1; src=1, dst=2, \n\t morph={'tokid': [2, 3], 'surface': ['ここ', 'で'], 'base': ['ここ', 'で'], 'pos': ['名詞', '助詞'], 'pos1': ['代名詞', '格助詞']}\nchunkid=2; src=2, dst=3, \n\t morph={'tokid': [4, 5], 'surface': ['始め', 'て'], 'base': ['始める', 'て'], 'pos': ['動詞', '助詞'], 'pos1': ['自立', '接続助詞']}\nchunkid=3; src=3, dst=4, \n\t morph={'tokid': [6, 7], 'surface': ['人間', 'という'], 'base': ['人間', 'という'], 'pos': ['名詞', '助詞'], 'pos1': ['一般', '格助詞']}\nchunkid=4; src=4, dst=5, \n\t morph={'tokid': [8, 9], 'surface': ['もの', 'を'], 'base': ['もの', 'を'], 'pos': ['名詞', '助詞'], 'pos1': ['非自立', '格助詞']}\nchunkid=5; src=5, dst=-1, \n\t morph={'tokid': [10, 11, 12], 'surface': ['見', 'た', '。'], 'base': ['見る', 'た', '。'], 'pos': ['動詞', '助動詞', '記号'], 'pos1': ['自立', '*', '句点']}\n"
    }
   ],
   "source": [
    "def knock_041():\n",
    "    with open('./section05/neko.txt.cabocha', 'rt', encoding='utf-8') as f:\n",
    "        sentences = f.read().replace('<sentence>', '').split('</sentence>')\n",
    "\n",
    "    neko_chunks = [\n",
    "        Chunk(sentence) for sentence in sentences\n",
    "        ]\n",
    "    \n",
    "    for chu in neko_chunks:\n",
    "        chu()\n",
    "\n",
    "    print_chunks(neko_chunks[7].chunks)\n",
    "\n",
    "knock_041()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT言語)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "```python:\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が$i$\n",
    "と$j$（$i<j$）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節$i$と$j$に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節$i$から構文木の根に至る経路上に文節$j$が存在する場合: 文節$i$から文節$j$のパスを表示\n",
    "- 上記以外で，文節$i$と文節$j$から構文木の根に至る経路上で共通の文節$k$で交わる場合: 文節$i$から文節$k$に至る直前のパスと文節$j$から文節$k$に至る直前までのパス，文節$k$の内容を” | “で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第６章 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開している[News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)を用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. データの入手・整形\n",
    "[News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)をダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "1. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "1. 抽出された事例をランダムに並び替える．\n",
    "1. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれ`train.txt`，`valid.txt`，`test.txt`というファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれ`train.feature.txt`，`valid.feature.txt`，`test.feature.txt`というファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 52. 学習\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. 予測\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. 正解率の計測\n",
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. 混同行列の作成\n",
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. 適合率，再現率，F1スコアの計測\n",
    "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 57. 特徴量の重みの確認\n",
    "52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58. 正則化パラメータの変更\n",
    "ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59. ハイパーパラメータの探索\n",
    "学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第７章 単語ベクトル\n",
    "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. 単語ベクトルの読み込みと表示\n",
    "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 61. 単語の類似度\n",
    "“United States”と”U.S.”のコサイン類似度を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 62. 類似度の高い単語10件\n",
    "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 63. 加法構成性によるアナロジー\n",
    "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64. アナロジーデータでの実験\n",
    "[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 65. アナロジータスクでの正解率\n",
    "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66. WordSimilarity-353での評価\n",
    "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 67. k-meansクラスタリング\n",
    "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 68. Ward法によるクラスタリング\n",
    "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 69. t-SNEによる可視化\n",
    "国名に関する単語ベクトルのベクトル空間をt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第８章 ニューラルネット\n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$\\boldsymbol{X}$\n",
    "と，正解ラベルを並べた行列（ベクトル）$\\boldsymbol{Y}$を作成したい．\n",
    "\n",
    "$$\n",
    "  X = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\boldsymbol{x}_1 \\\\\n",
    "      \\boldsymbol{x}_2 \\\\\n",
    "      \\dots \\\\\n",
    "      \\boldsymbol{x}_4\n",
    "    \\end{array}\n",
    "  \\right) \\in \\mathbb{R}^{n \\times d},　\n",
    "  Y = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      \\dots \\\\\n",
    "      y_4\n",
    "    \\end{array}\n",
    "  \\right) \\in \\mathbb{N}^{n}\n",
    "$$\n",
    " \n",
    "ここで，$n$は学習データの事例数であり，$\\boldsymbol{x}_{i} \\in \\mathbb{R}^{d}$と$y_{i} \\in \\mathbb{N}$はそれぞれ，${i} \\in \\{1, 2, ..., n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb{N}_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_{i} \\in \\mathbb{N}_{<4}$で表現できる． 以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
    "\n",
    "i番目の事例の特徴ベクトル$\\boldsymbol{x}_i$は，次式で求める．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_i = \\frac{1}{T_i} \\Sigma_{t=1}^{T_i} {{\\rm emb}(w_{i,t})}\n",
    "$$\n",
    "\n",
    "ここで，i番目の事例は$T_i$個の（記事見出しの）単語列($w_{i,1},w_{i,2},…,w_{i,Ti}$)から構成され，${\\rm emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数はd）である．すなわち，i番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$x_i$である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．300次元の単語ベクトルを用いたので，d=300である．\n",
    "\n",
    "i番目の事例のラベル$y_i$は，次のように定義する．\n",
    "\n",
    "$$\n",
    "y_i = \\left\\{ \\begin{array}{ll}\n",
    "    0 & (記事 \\boldsymbol{x}_i が「ビジネス」カテゴリの場合) \\\\\n",
    "    1 & (記事 \\boldsymbol{x}_i が「科学技術」カテゴリの場合) \\\\\n",
    "    2 & (記事 \\boldsymbol{x}_i が「エンターテイメント」カテゴリの場合) \\\\\n",
    "    3 & (記事 \\boldsymbol{x}_i が「健康」カテゴリの場合) \\\\\n",
    "    \\end{array} \\right.\n",
    "$$\n",
    " \n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
    "\n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "\n",
    "- 学習データの特徴量行列: $ \\boldsymbol{X}_{\\rm train} \\in \\mathbb{R}^{N_t \\times d} $\n",
    "- 学習データのラベルベクトル: $ \\boldsymbol{Y}_{\\rm train} \\in \\mathbb{N}^{N_t} $\n",
    "- 検証データの特徴量行列: $ \\boldsymbol{X}_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d} $\n",
    "- 検証データのラベルベクトル: $ \\boldsymbol{Y}_{\\rm valid} \\in \\mathbb{N}^{N_v} $\n",
    "- 評価データの特徴量行列: $ \\boldsymbol{X}_{\\rm test} \\in \\mathbb{R}^{N_e \\times d} $\n",
    "- 評価データのラベルベクトル: $ \\boldsymbol{Y}_{\\rm test} \\in \\mathbb{N}^{N_e} $\n",
    "\n",
    "なお，$N_t$, $N_v$, $N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "$$\n",
    "\\hat{y_1}={\\rm softmax} (\\boldsymbol{x}_1 W),　\\hat{Y} ={\\rm softmax} (X_{[1:4]} W)\n",
    "$$\n",
    "\n",
    "ただし，softmaxはソフトマックス関数，$X_{[1:4]} \\in \\mathbb{R}^{4 \\times d}$ は特徴ベクトル$\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3, \\boldsymbol{x}_4$を縦に並べた行列である．\n",
    "\n",
    "$$\n",
    "  X_{[1:4]} = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\boldsymbol{x}_1 \\\\\n",
    "      \\boldsymbol{x}_2 \\\\\n",
    "      \\boldsymbol{x}_3 \\\\\n",
    "      \\boldsymbol{x}_4\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "行列 $W \\in \\mathbb{R}^{d \\times L}$ は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，$\\hat{y_1} \\in \\mathbb{N}^L$ は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトルである． 同様に，$ \\hat{Y} \\in \\mathbb{N}_n^L$ は，学習データの事例$x_1,x_2,x_3,x_4$について，各カテゴリに属する確率を行列として表現している．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算\n",
    "学習データの事例$x_1$と事例集合$x_1$,$x_2$,$x_3$,$x_4$に対して，クロスエントロピー損失と，行列$W$に対する勾配を計算せよ．なお，ある事例$x_i$\n",
    "に対して損失は次式で計算される．\n",
    "\n",
    "$$\n",
    "l_i = − {\\rm log}[事例 x_i が y_i に分類される確率]\n",
    "$$\n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列$W$を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. チェックポイント\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. ミニバッチ化\n",
    "問題76のコードを改変し，$ \\boldsymbol{B}$ 事例ごとに損失・勾配を計算し，行列$ \\boldsymbol{W}$の値を更新せよ（ミニバッチ化）．$ \\boldsymbol{B}$の値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. GPU上での学習\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 多層ニューラルネットワーク\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第９章 RNNとCNN\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. RNNによる予測\n",
    "ID番号で表現された単語列 $\\boldsymbol{x}=(x_1,x_2,…,x_T)$\n",
    "がある．ただし，$T$\n",
    "は単語列の長さ，$x_t \\in \\mathbb{R}^{V} $\n",
    "は単語のID番号のone-hot表記である（ $V$\n",
    "は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列$ \\boldsymbol{x} $\n",
    "からカテゴリ$y$\n",
    "を予測するモデルとして，次式を実装せよ．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_0 = 0 \\\\\n",
    "\\overrightarrow{h}_t = \\overrightarrow{\\rm RNN} ({\\rm emb}(x_t), \\overrightarrow{h}_{t-1}) \\\\\n",
    "y = {\\rm softmax}( \\boldsymbol{W}^{(yh)} \\overrightarrow{h}_t + b^{(y)}\n",
    "$$\n",
    "\n",
    "ただし，${\\rm emb}(x_t) \\in \\mathbb{R}^{d_w} $\n",
    "は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），$ \\overrightarrow{h}_t \\in \\mathbb{R}^{d_h} $\n",
    "は時刻$t$\n",
    "の隠れ状態ベクトル，$ \\overrightarrow{\\rm RNN}(x,h) $\n",
    "は入力$x$\n",
    "と前時刻の隠れ状態$h$\n",
    "から次状態を計算するRNNユニット，$ W^{(yh)} \\in \\mathbb{R}^{L \\times d_h} $\n",
    "は隠れ状態ベクトルからカテゴリを予測するための行列，$ b^{(y)}\\in \\mathbb{R}^{L} $\n",
    "はバイアス項である（$ d_w,d_h,L $\n",
    "はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット $ \\overrightarrow{\\rm RNN}(x,h) $\n",
    "には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\rm RNN}(x,h) = \n",
    "    g(\\boldsymbol{W}^{(hx)} x + \\boldsymbol{W}^{(hh)} h + b^{(h)})\n",
    "$$\n",
    "ただし，$ \\boldsymbol{W}^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}, \\boldsymbol{W}^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h} $\n",
    "はRNNユニットのパラメータ，$g$\n",
    "は活性化関数（例えば$ \\rm tanh $\n",
    "やReLUなど）である．\n",
    "\n",
    "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでy\n",
    "を計算するだけでよい．次元数などのハイパーパラメータは，$d_w=300,d_h=50$\n",
    "など，適当な値に設定せよ（以降の問題でも同様である）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，$B$\n",
    "事例ごとに損失・勾配を計算して学習を行えるようにせよ（$B$の値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット(約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込み$ {\\rm emb}(x)$を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_{T+1} = 0 \\\\\n",
    "\\overrightarrow{h}_t = \\overrightarrow{\\rm RNN} ({\\rm emb}(x_t), \\overrightarrow{h}_{t+1}) \\\\\n",
    "y = {\\rm softmax} \\bigl( \\boldsymbol{W}^{(yh)} \\bigl[ \\overrightarrow{h}_T ; \\overleftarrow{h}_1 \\bigr] + b^{(y)} \\bigr)\n",
    "$$\n",
    "\n",
    "ただし，$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h} $\n",
    "はそれぞれ，順方向および逆方向のRNNで求めた時刻$t$\n",
    "の隠れ状態ベクトル，$ \\overrightarrow{\\rm RNN}(x,h) $\n",
    "は入力$x$\n",
    "と次時刻の隠れ状態$h$\n",
    "から前状態を計算するRNNユニット，$ \\boldsymbol{W}^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h} $\n",
    "は隠れ状態ベクトルからカテゴリを予測するための行列，$ b^{(y)} \\in \\mathbb{R}^{L} $\n",
    "はバイアス項である．また，$[a;b]$\n",
    "はベクトル$a$\n",
    "と$b$\n",
    "の連結を表す。\n",
    "\n",
    "さらに，双方向RNNを多層化して実験せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列$ \\boldsymbol{x} =(x_1,x_2,…,x_T) $\n",
    "がある．ただし，$T$\n",
    "は単語列の長さ，$x_t \\in \\mathbb{R}^{V} $\n",
    "は単語のID番号のone-hot表記である（$V$\n",
    "は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$\\boldsymbol{x}$\n",
    "からカテゴリ$y$\n",
    "を予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "- 単語埋め込みの次元数: $d_w$\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: $d_h$\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$d_h$次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻$t$\n",
    "の特徴ベクトル$p_t \\in \\mathbb{R}^{d_h} $\n",
    "は次式で表される．\n",
    "$$\n",
    "p_t g \\bigl( \n",
    "    \\boldsymbol{W}^{(px)} \\bigl[ {\\rm emb}(x_{t-1}) ; {\\rm emb}(x_{t}) ; {\\rm emb}(x_{t+1}) \\bigr] + b^{(p)}\n",
    "    \\bigr)\n",
    "$$\n",
    "\n",
    "ただし，$ \\boldsymbol{W}^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h} $\n",
    "はCNNのパラメータ，$g$\n",
    "は活性化関数（例えば$ \\rm tanh$\n",
    "やReLUなど），$[a;b;c]$\n",
    "はベクトル$a,b,c$\n",
    "の連結である．なお，行列$ \\boldsymbol{W}^{(px)} $\n",
    "の列数が$3d_w$\n",
    "になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル $ c \\in \\mathbb{R}^{d_h} $\n",
    "を求める．$c[i]$\n",
    "でベクトル$c$\n",
    "の$i$\n",
    "番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "$$\n",
    "c[i] = \\underset{1 \\leq t \\leq T}{\\rm max} p_t[i]\n",
    "$$\n",
    "\n",
    "最後に，入力文書の特徴ベクトル$c$\n",
    "に行列$ \\boldsymbol{W}^{(yc)} \\in \\mathbb{R}^{L \\times d_h} $\n",
    "とバイアス項$ b^{(y)} \\in \\mathbb{R}^L $\n",
    "による線形変換とソフトマックス関数を適用し，カテゴリ$y$\n",
    "を予測する．\n",
    "\n",
    "$$\n",
    "y = {\\rm softmax} \\bigl( \\boldsymbol{W}^{(yc)} c + b^{(y)} \\bigr)\n",
    "$$\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$y$\n",
    "を計算するだけでよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えば[BERT](https://github.com/google-research/bert)など）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第１０章 機械翻訳\n",
    "本章では，日本語と英語の翻訳コーパスである[京都フリー翻訳タスク (KFTT)](http://www.phontron.com/kftt/index-ja.html)を用い，ニューラル機械翻訳モデルを構築する．ニューラル機械翻訳モデルの構築には，[fairseq](https://github.com/pytorch/fairseq)，[Hugging Face Transformers](https://github.com/huggingface/transformers)，[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)などの既存のツールを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. 学習過程の可視化\n",
    "[Tensorboard](https://www.tensorflow.org/tensorboard)などのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. ドメイン適応\n",
    "[Japanese-English Subtitle Corpus (JESC)](https://nlp.stanford.edu/projects/jesc/index_ja.html)や[JParaCrawl](http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/)などの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}