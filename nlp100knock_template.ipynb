{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP; Natural Language Processing 100 KNOCKs!!!!\n",
    "https://nlp100.github.io/ja/\n",
    "## 配布しているデータについて\n",
    "- baby-names.txt: 米国社会保障局 (SSA: Social Security Administration)のウェブサイト”Beyond the Top 1000 Names“で公開されている全州のデータを加工し，TSV形式に変換したもの．\n",
    "- jawiki-country.json.gz: 2020年4月5日付けの日本語のWikipedia記事のダンプの中から，国家に言及していると思われる記事を抽出し，JSON形式で格納したものです．このファイルは，クリエイティブ・コモンズ 表示-継承 3.0 非移植のライセンスで配布されています．\n",
    "- neko.txt: 青空文庫で公開されている夏目漱石の長編小説『吾輩は猫である』をテキストファイルに整形したものです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第１章 準備運動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. 文字列の逆順 \n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 「パタトクカシーー」 \n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」 \n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 円周率 \n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 元素記号 \n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭に2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. n-gram \n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. 集合 \n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. テンプレートによる文生成 \n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. 暗号文 \n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "\n",
    "- 英小文字ならば(219 - 文字コード)の文字に置換\n",
    "- その他の文字はそのまま出力\n",
    "- この関数を用い，英語のメッセージを暗号化・復号化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Typoglycemia \n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第２章 UNIXコマンド\n",
    "\n",
    "[popular-names.txt](https://nlp100.github.io/data/popular-names.txt)は，アメリカで生まれた赤ちゃんの「名前」「性別」「人数」「年」をタブ区切り形式で格納したファイルである．以下の処理を行うプログラムを作成し，[popular-names.txt](https://nlp100.github.io/data/popular-names.txt)を入力ファイルとして実行せよ．さらに，同様の処理をUNIXコマンドでも実行し，プログラムの実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 行数のカウント \n",
    "行数をカウントせよ．確認にはwcコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. タブをスペースに置換 \n",
    "タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 1列目をcol1.txtに，2列目をcol2.txtに保存 \n",
    "各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. col1.txtとcol2.txtをマージ \n",
    "12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 先頭からN行を出力 \n",
    "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 末尾のN行を出力 \n",
    "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ファイルをN分割する \n",
    "自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. １列目の文字列の異なり \n",
    "1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはcut, sort, uniqコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 各行を3コラム目の数値の降順にソート \n",
    "各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる \n",
    "各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第３章 正規表現\n",
    "\n",
    "Wikipediaの記事を以下のフォーマットで書き出したファイル[jawiki-country.json.gz](https://nlp100.github.io/data/jawiki-country.json.gz)がある．\n",
    "\n",
    "- 1行に1記事の情報がJSON形式で格納される\n",
    "- 各行には記事名が”title”キーに，記事本文が”text”キーの辞書オブジェクトに格納され，そのオブジェクトがJSON形式で書き出される\n",
    "- ファイル全体はgzipで圧縮される  \n",
    "\n",
    "以下の処理を行うプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. JSONデータの読み込み \n",
    "Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．問題21-29では，ここで抽出した記事本文に対して実行せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. カテゴリ名を含む行を抽出 \n",
    "記事中でカテゴリ名を宣言している行を抽出せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. カテゴリ名の抽出 \n",
    "記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. セクション構造 \n",
    "記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. ファイル参照の抽出 \n",
    "記事から参照されているメディアファイルをすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. テンプレートの抽出 \n",
    "記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. 強調マークアップの除去 \n",
    "25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ（参考: [マークアップ早見表](http://ja.wikipedia.org/wiki/Help:早見表)）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. 内部リンクの除去 \n",
    "26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: [マークアップ早見表](http://ja.wikipedia.org/wiki/Help:早見表)）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. MediaWikiマークアップの除去 \n",
    "27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29. 国旗画像のURLを取得する \n",
    "テンプレートの内容を利用し，国旗画像のURLを取得せよ．（ヒント: [MediaWiki API](http://www.mediawiki.org/wiki/API:Main_page/ja)の[imageinfo](https://www.mediawiki.org/wiki/API:Imageinfo)を呼び出して，ファイル参照をURLに変換すればよい）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第４章 形態素解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
    "\n",
    "なお，問題37, 38, 39は[matplotlib](http://matplotlib.org/)もしくは[Gnuplot](Gnuplot)を用いるとよい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. 形態素解析結果の読み込み \n",
    "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. 動詞 \n",
    "動詞の表層形をすべて抽出せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. 動詞の原形 \n",
    "動詞の原形をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. 「AのB」 \n",
    "2つの名詞が「の」で連結されている名詞句を抽出せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. 名詞の連接\n",
    "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. 単語の出現頻度\n",
    "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36. 頻度上位10語\n",
    "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37. 「猫」と共起頻度の高い上位10語\n",
    "「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38. ヒストグラム\n",
    "単語の出現頻度のヒストグラム（横軸に出現頻度，縦軸に出現頻度をとる単語の種類数を棒グラフで表したもの）を描け．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39. Zipfの法則\n",
    "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第５章 係り受け解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT言語)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "```python:\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が$i$\n",
    "と$j$（$i<j$）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節$i$と$j$に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節$i$から構文木の根に至る経路上に文節$j$が存在する場合: 文節$i$から文節$j$のパスを表示\n",
    "- 上記以外で，文節$i$と文節$j$から構文木の根に至る経路上で共通の文節$k$で交わる場合: 文節$i$から文節$k$に至る直前のパスと文節$j$から文節$k$に至る直前までのパス，文節$k$の内容を” | “で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第６章 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開している[News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)を用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. データの入手・整形\n",
    "[News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)をダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "1. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "1. 抽出された事例をランダムに並び替える．\n",
    "1. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれ`train.txt`，`valid.txt`，`test.txt`というファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれ`train.feature.txt`，`valid.feature.txt`，`test.feature.txt`というファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 52. 学習\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. 予測\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. 正解率の計測\n",
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. 混同行列の作成\n",
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. 適合率，再現率，F1スコアの計測\n",
    "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 57. 特徴量の重みの確認\n",
    "52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58. 正則化パラメータの変更\n",
    "ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59. ハイパーパラメータの探索\n",
    "学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第７章 単語ベクトル\n",
    "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. 単語ベクトルの読み込みと表示\n",
    "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 61. 単語の類似度\n",
    "“United States”と”U.S.”のコサイン類似度を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 62. 類似度の高い単語10件\n",
    "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 63. 加法構成性によるアナロジー\n",
    "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64. アナロジーデータでの実験\n",
    "[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 65. アナロジータスクでの正解率\n",
    "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66. WordSimilarity-353での評価\n",
    "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 67. k-meansクラスタリング\n",
    "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 68. Ward法によるクラスタリング\n",
    "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 69. t-SNEによる可視化\n",
    "国名に関する単語ベクトルのベクトル空間をt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第８章 ニューラルネット\n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$\\boldsymbol{X}$\n",
    "と，正解ラベルを並べた行列（ベクトル）$\\boldsymbol{Y}$を作成したい．\n",
    "\n",
    "$$\n",
    "  X = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\boldsymbol{x}_1 \\\\\n",
    "      \\boldsymbol{x}_2 \\\\\n",
    "      \\dots \\\\\n",
    "      \\boldsymbol{x}_4\n",
    "    \\end{array}\n",
    "  \\right) \\in \\mathbb{R}^{n \\times d},　\n",
    "  Y = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      \\dots \\\\\n",
    "      y_4\n",
    "    \\end{array}\n",
    "  \\right) \\in \\mathbb{N}^{n}\n",
    "$$\n",
    " \n",
    "ここで，$n$は学習データの事例数であり，$\\boldsymbol{x}_{i} \\in \\mathbb{R}^{d}$と$y_{i} \\in \\mathbb{N}$はそれぞれ，${i} \\in \\{1, 2, ..., n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb{N}_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_{i} \\in \\mathbb{N}_{<4}$で表現できる． 以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
    "\n",
    "i番目の事例の特徴ベクトル$\\boldsymbol{x}_i$は，次式で求める．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_i = \\frac{1}{T_i} \\Sigma_{t=1}^{T_i} {{\\rm emb}(w_{i,t})}\n",
    "$$\n",
    "\n",
    "ここで，i番目の事例は$T_i$個の（記事見出しの）単語列($w_{i,1},w_{i,2},…,w_{i,Ti}$)から構成され，${\\rm emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数はd）である．すなわち，i番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$x_i$である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．300次元の単語ベクトルを用いたので，d=300である．\n",
    "\n",
    "i番目の事例のラベル$y_i$は，次のように定義する．\n",
    "\n",
    "$$\n",
    "y_i = \\left\\{ \\begin{array}{ll}\n",
    "    0 & (記事 \\boldsymbol{x}_i が「ビジネス」カテゴリの場合) \\\\\n",
    "    1 & (記事 \\boldsymbol{x}_i が「科学技術」カテゴリの場合) \\\\\n",
    "    2 & (記事 \\boldsymbol{x}_i が「エンターテイメント」カテゴリの場合) \\\\\n",
    "    3 & (記事 \\boldsymbol{x}_i が「健康」カテゴリの場合) \\\\\n",
    "    \\end{array} \\right.\n",
    "$$\n",
    " \n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
    "\n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "\n",
    "- 学習データの特徴量行列: $ \\boldsymbol{X}_{\\rm train} \\in \\mathbb{R}^{N_t \\times d} $\n",
    "- 学習データのラベルベクトル: $ \\boldsymbol{Y}_{\\rm train} \\in \\mathbb{N}^{N_t} $\n",
    "- 検証データの特徴量行列: $ \\boldsymbol{X}_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d} $\n",
    "- 検証データのラベルベクトル: $ \\boldsymbol{Y}_{\\rm valid} \\in \\mathbb{N}^{N_v} $\n",
    "- 評価データの特徴量行列: $ \\boldsymbol{X}_{\\rm test} \\in \\mathbb{R}^{N_e \\times d} $\n",
    "- 評価データのラベルベクトル: $ \\boldsymbol{Y}_{\\rm test} \\in \\mathbb{N}^{N_e} $\n",
    "\n",
    "なお，$N_t$, $N_v$, $N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "$$\n",
    "\\hat{y_1}={\\rm softmax} (\\boldsymbol{x}_1 W),　\\hat{Y} ={\\rm softmax} (X_{[1:4]} W)\n",
    "$$\n",
    "\n",
    "ただし，softmaxはソフトマックス関数，$X_{[1:4]} \\in \\mathbb{R}^{4 \\times d}$ は特徴ベクトル$\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3, \\boldsymbol{x}_4$を縦に並べた行列である．\n",
    "\n",
    "$$\n",
    "  X_{[1:4]} = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\boldsymbol{x}_1 \\\\\n",
    "      \\boldsymbol{x}_2 \\\\\n",
    "      \\boldsymbol{x}_3 \\\\\n",
    "      \\boldsymbol{x}_4\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "行列 $W \\in \\mathbb{R}^{d \\times L}$ は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，$\\hat{y_1} \\in \\mathbb{N}^L$ は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトルである． 同様に，$ \\hat{Y} \\in \\mathbb{N}_n^L$ は，学習データの事例$x_1,x_2,x_3,x_4$について，各カテゴリに属する確率を行列として表現している．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算\n",
    "学習データの事例$x_1$と事例集合$x_1$,$x_2$,$x_3$,$x_4$に対して，クロスエントロピー損失と，行列$W$に対する勾配を計算せよ．なお，ある事例$x_i$\n",
    "に対して損失は次式で計算される．\n",
    "\n",
    "$$\n",
    "l_i = − {\\rm log}[事例 x_i が y_i に分類される確率]\n",
    "$$\n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列$W$を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. チェックポイント\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. ミニバッチ化\n",
    "問題76のコードを改変し，$ \\boldsymbol{B}$ 事例ごとに損失・勾配を計算し，行列$ \\boldsymbol{W}$の値を更新せよ（ミニバッチ化）．$ \\boldsymbol{B}$の値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. GPU上での学習\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 多層ニューラルネットワーク\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第９章 RNNとCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. RNNによる予測\n",
    "ID番号で表現された単語列 $\\boldsymbol{x}=(x_1,x_2,…,x_T)$\n",
    "がある．ただし，$T$\n",
    "は単語列の長さ，$x_t \\in \\mathbb{R}^{V} $\n",
    "は単語のID番号のone-hot表記である（ $V$\n",
    "は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列$ \\boldsymbol{x} $\n",
    "からカテゴリ$y$\n",
    "を予測するモデルとして，次式を実装せよ．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_0 = 0 \\\\\n",
    "\\overrightarrow{h}_t = \\overrightarrow{\\rm RNN} ({\\rm emb}(x_t), \\overrightarrow{h}_{t-1}) \\\\\n",
    "y = {\\rm softmax}( \\boldsymbol{W}^{(yh)} \\overrightarrow{h}_t + b^{(y)}\n",
    "$$\n",
    "\n",
    "ただし，${\\rm emb}(x_t) \\in \\mathbb{R}^{d_w} $\n",
    "は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），$ \\overrightarrow{h}_t \\in \\mathbb{R}^{d_h} $\n",
    "は時刻$t$\n",
    "の隠れ状態ベクトル，$ \\overrightarrow{\\rm RNN}(x,h) $\n",
    "は入力$x$\n",
    "と前時刻の隠れ状態$h$\n",
    "から次状態を計算するRNNユニット，$ W^{(yh)} \\in \\mathbb{R}^{L \\times d_h} $\n",
    "は隠れ状態ベクトルからカテゴリを予測するための行列，$ b^{(y)}\\in \\mathbb{R}^{L} $\n",
    "はバイアス項である（$ d_w,d_h,L $\n",
    "はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット $ \\overrightarrow{\\rm RNN}(x,h) $\n",
    "には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\rm RNN}(x,h) = \n",
    "    g(\\boldsymbol{W}^{(hx)} x + \\boldsymbol{W}^{(hh)} h + b^{(h)})\n",
    "$$\n",
    "ただし，$ \\boldsymbol{W}^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}, \\boldsymbol{W}^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h} $\n",
    "はRNNユニットのパラメータ，$g$\n",
    "は活性化関数（例えば$ \\rm tanh $\n",
    "やReLUなど）である．\n",
    "\n",
    "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでy\n",
    "を計算するだけでよい．次元数などのハイパーパラメータは，$d_w=300,d_h=50$\n",
    "など，適当な値に設定せよ（以降の問題でも同様である）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，$B$\n",
    "事例ごとに損失・勾配を計算して学習を行えるようにせよ（$B$の値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット(約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込み$ {\\rm emb}(x)$を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_{T+1} = 0 \\\\\n",
    "\\overrightarrow{h}_t = \\overrightarrow{\\rm RNN} ({\\rm emb}(x_t), \\overrightarrow{h}_{t+1}) \\\\\n",
    "y = {\\rm softmax} \\bigl( \\boldsymbol{W}^{(yh)} \\bigl[ \\overrightarrow{h}_T ; \\overleftarrow{h}_1 \\bigr] + b^{(y)} \\bigr)\n",
    "$$\n",
    "\n",
    "ただし，$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h} $\n",
    "はそれぞれ，順方向および逆方向のRNNで求めた時刻$t$\n",
    "の隠れ状態ベクトル，$ \\overrightarrow{\\rm RNN}(x,h) $\n",
    "は入力$x$\n",
    "と次時刻の隠れ状態$h$\n",
    "から前状態を計算するRNNユニット，$ \\boldsymbol{W}^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h} $\n",
    "は隠れ状態ベクトルからカテゴリを予測するための行列，$ b^{(y)} \\in \\mathbb{R}^{L} $\n",
    "はバイアス項である．また，$[a;b]$\n",
    "はベクトル$a$\n",
    "と$b$\n",
    "の連結を表す。\n",
    "\n",
    "さらに，双方向RNNを多層化して実験せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列$ \\boldsymbol{x} =(x_1,x_2,…,x_T) $\n",
    "がある．ただし，$T$\n",
    "は単語列の長さ，$x_t \\in \\mathbb{R}^{V} $\n",
    "は単語のID番号のone-hot表記である（$V$\n",
    "は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$\\boldsymbol{x}$\n",
    "からカテゴリ$y$\n",
    "を予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "- 単語埋め込みの次元数: $d_w$\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: $d_h$\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$d_h$次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻$t$\n",
    "の特徴ベクトル$p_t \\in \\mathbb{R}^{d_h} $\n",
    "は次式で表される．\n",
    "$$\n",
    "p_t g \\bigl( \n",
    "    \\boldsymbol{W}^{(px)} \\bigl[ {\\rm emb}(x_{t-1}) ; {\\rm emb}(x_{t}) ; {\\rm emb}(x_{t+1}) \\bigr] + b^{(p)}\n",
    "    \\bigr)\n",
    "$$\n",
    "\n",
    "ただし，$ \\boldsymbol{W}^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h} $\n",
    "はCNNのパラメータ，$g$\n",
    "は活性化関数（例えば$ \\rm tanh$\n",
    "やReLUなど），$[a;b;c]$\n",
    "はベクトル$a,b,c$\n",
    "の連結である．なお，行列$ \\boldsymbol{W}^{(px)} $\n",
    "の列数が$3d_w$\n",
    "になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル $ c \\in \\mathbb{R}^{d_h} $\n",
    "を求める．$c[i]$\n",
    "でベクトル$c$\n",
    "の$i$\n",
    "番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "$$\n",
    "c[i] = \\underset{1 \\leq t \\leq T}{\\rm max} p_t[i]\n",
    "$$\n",
    "\n",
    "最後に，入力文書の特徴ベクトル$c$\n",
    "に行列$ \\boldsymbol{W}^{(yc)} \\in \\mathbb{R}^{L \\times d_h} $\n",
    "とバイアス項$ b^{(y)} \\in \\mathbb{R}^L $\n",
    "による線形変換とソフトマックス関数を適用し，カテゴリ$y$\n",
    "を予測する．\n",
    "\n",
    "$$\n",
    "y = {\\rm softmax} \\bigl( \\boldsymbol{W}^{(yc)} c + b^{(y)} \\bigr)\n",
    "$$\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$y$\n",
    "を計算するだけでよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えば[BERT](https://github.com/google-research/bert)など）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 第１０章 機械翻訳\n",
    "本章では，日本語と英語の翻訳コーパスである[京都フリー翻訳タスク (KFTT)](http://www.phontron.com/kftt/index-ja.html)を用い，ニューラル機械翻訳モデルを構築する．ニューラル機械翻訳モデルの構築には，[fairseq](https://github.com/pytorch/fairseq)，[Hugging Face Transformers](https://github.com/huggingface/transformers)，[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)などの既存のツールを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. 学習過程の可視化\n",
    "[Tensorboard](https://www.tensorflow.org/tensorboard)などのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. ドメイン適応\n",
    "[Japanese-English Subtitle Corpus (JESC)](https://nlp.stanford.edu/projects/jesc/index_ja.html)や[JParaCrawl](http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/)などの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
